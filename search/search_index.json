{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"functime","text":""},{"location":"#production-ready-time-series-models","title":"Production-ready time series models","text":"<p>functime is a machine learning library for time-series predictions that just works.</p> <ul> <li>Fully-featured: Powerful and easy-to-use API for forecasting and feature engineering (tsfresh, Catch22).</li> <li>Fast: Forecast and classify 100,000 time series in seconds on your laptop</li> <li>Cloud-native: Instantly run, deploy, and serve predictive time-series models</li> <li>Efficient: Embarressingly parallel feature engineering using Polars *</li> <li>Battle-tested: Algorithms that deliver real business impact and win competitions</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Check out this guide to install functime. Requires Python 3.8+.</p>"},{"location":"#forecasting","title":"Forecasting","text":"<p>Point and probablistic forecasts using machine learning. Includes utilities to support the full forecasting lifecycle: preprocessing, feature extraction, time-series cross-validation / splitters, backtesting, automated hyperparameter tuning, and scoring.</p> <ul> <li>Every forecaster supports exogenous features</li> <li>Backtesting with expanding window and sliding window splitters</li> <li>Automated lags and hyperparameter tuning using <code>FLAML</code></li> <li>Probablistic forecasts via quantile regression and conformal prediction</li> <li>Forecast metrics (e.g. MASE, SMAPE, CRPS) for scoring in parallel</li> <li>Supports recursive and direct forecast strategies</li> <li>Censored model for zero-inflated forecasts</li> </ul> <p>View the full walkthrough on forecasting with <code>functime</code>.</p>"},{"location":"#quick-examples","title":"Quick Examples","text":"<p>Input Data Schemas</p> <p>Forecasters, preprocessors, and splitters take a panel dataset where the first two columns represent entity (e.g. commodty name) and time (e.g. date). Subsequent columns represent observed values (e.g. price). The panel DataFrame must be sorted by entity, time.</p> <pre><code>&gt;&gt;&gt; y_panel\nshape: (47_583, 3)\n\ncommodity_type   time         price\n------------------------------------\nAluminum         1960-01-01    511.47\n                 1960-02-01    511.47\n                 1960-03-01    511.47\n                 1960-04-01    511.47\n                 1960-05-01    511.47\n...                     ...       ...\nZinc             2022-11-01   2938.92\n                 2022-12-01   3129.48\n                 2023-01-01   3309.81\n                 2023-02-01   3133.84\n                 2023-03-01   2967.46\n</code></pre>"},{"location":"#forecasting_1","title":"Forecasting","text":"<pre><code>import polars as pl\nfrom functime.cross_validation import train_test_split\nfrom functime.feature_extraction import add_fourier_terms\nfrom functime.forecasting import linear_model\nfrom functime.preprocessing import scale\nfrom functime.metrics import mase\n# Load commodities price data\ny = pl.read_parquet(\"https://github.com/descendant-ai/functime/raw/main/data/commodities.parquet\")\nentity_col, time_col = y.columns[:2]\n# Time series split\ny_train, y_test = y.pipe(train_test_split(test_size=3))\n# Fit-predict\nforecaster = linear_model(freq=\"1mo\", lags=24)\nforecaster.fit(y=y_train)\ny_pred = forecaster.predict(fh=3)\n# functime \u2764\ufe0f functional design\n# fit-predict in a single line\ny_pred = linear_model(freq=\"1mo\", lags=24)(y=y_train, fh=3)\n# Score forecasts in parallel\nscores = mase(y_true=y_test, y_pred=y_pred, y_train=y_train)\n# Forecast with target transforms and feature transforms\nforecaster = linear_model(\nfreq=\"1mo\",\nlags=24,\ntarget_transform=scale(),\nfeature_transform=add_fourier_terms(sp=12, K=6)\n)\n</code></pre>"},{"location":"#splitters","title":"Splitters","text":"<p>View API reference for <code>functime.cross_validation</code>. <code>functime</code> currently supports expanding window and rolling window splitters. Splitters are used for cross-validation and backtesting.</p>"},{"location":"#preprocessing","title":"Preprocessing","text":"<p>View API reference for <code>functime.preprocessing</code>. Preprocessors take in a <code>polars.DataFrame</code> or <code>polars.LazyFrame</code> as input and always returns a <code>polars.LazyFrame</code>. No computation is run until the collect() method is called on the LazyFrame. This allows Polars to optimize the whole query before execution.</p> <pre><code>from functime.preprocessing import boxcox, impute\n# Use df.pipe to chain operations together\nX_new: pl.LazyFrame = (\nX.pipe(boxcox(method=\"mle\"))\n.pipe(detrend(method=\"linear\"))\n)\n# Call .collect to execute query\nX_new: pl.DataFrame = X_new.collect(streaming=True)\n</code></pre> <p>View quick examples of time-series preprocessing with <code>functime</code>.</p>"},{"location":"#time-series-data","title":"Time Series Data","text":""},{"location":"#feature-engineering","title":"Feature Engineering","text":"<p>Easily enrich your forecasts with calendar effects, holidays, weather patterns (coming soon), economic data (coming soon), and seasonality features (i.e. Fourier Series). View API reference for <code>functime.feature_extraction</code>.</p>"},{"location":"#example-data","title":"Example Data","text":"<p>It is easy to get started with <code>functime</code>. Our GitHub repo contains a growing number of time-series data stored as <code>parquet</code> files:</p> <ul> <li>M4 Competition (daily, weekly, monthly, quarterly, yearly)<sup>1</sup></li> <li>M5 Competition<sup>2</sup></li> <li>Australian tourism<sup>3</sup></li> <li>Commodities prices<sup>4</sup></li> <li>User laptop activity<sup>5</sup></li> <li>Gunpoint measurements<sup>6</sup></li> <li>Japanese vowels <sup>7</sup></li> </ul> <ol> <li> <p>https://mofc.unic.ac.cy/m4/\u00a0\u21a9</p> </li> <li> <p>https://mofc.unic.ac.cy/m5-competition/\u00a0\u21a9</p> </li> <li> <p>https://www.abs.gov.au/statistics/industry/tourism-and-transport/overseas-arrivals-and-departures-australia\u00a0\u21a9</p> </li> <li> <p>https://www.imf.org/en/Research/commodity-prices\u00a0\u21a9</p> </li> <li> <p>https://www.sciencedirect.com/science/article/pii/S2352340920306612\u00a0\u21a9</p> </li> <li> <p>http://www.timeseriesclassification.com/description.php?Dataset=GunPoint\u00a0\u21a9</p> </li> <li> <p>http://www.timeseriesclassification.com/description.php?Dataset=JapaneseVowels\u00a0\u21a9</p> </li> </ol>"},{"location":"develop/","title":"Developer Guide","text":"<p>This guide shows you how to use <code>functime</code>'s primitives to create new <code>forecasters</code> and <code>transformers</code>. If you would like to add your custom implementation into the <code>functime</code> library, please open up an draft pull request on GitHub! All contributions are welcome.</p>"},{"location":"develop/#build-your-own-forecaster","title":"Build your own <code>forecaster</code>","text":"<p>\ud83d\udea7 Under construction.</p>"},{"location":"develop/#build-your-own-transformer","title":"Build your own <code>transformer</code>","text":"<p><code>functime</code> provides an easy-to-use and functional <code>@transformer</code> decorator to implement new <code>transformers</code>. Here is an example:</p> <pre><code>@transformer\ndef lag(lags: List[int]):\n\"\"\"Applies lag transformation to a LazyFrame.\n    Parameters\n    ----------\n    lags : List[int]\n        A list of lag values to apply.\n    \"\"\"\ndef transform(X: pl.LazyFrame) -&gt; pl.LazyFrame:\nentity_col = X.columns[0]\ntime_col = X.columns[1]\nmax_lag = max(lags)\nlagged_series = [\n(\npl.all()\n.exclude([entity_col, time_col])\n.shift(lag)\n.over(entity_col)\n.suffix(f\"__lag_{lag}\")\n)\nfor lag in lags\n]\nX_new = (\n# Pre-sorting seems to improve performance by ~20%\nX.sort(by=[entity_col, time_col])\n.select(\npl.col(entity_col).set_sorted(),\npl.col(time_col).set_sorted(),\n*lagged_series,\n)\n.groupby(entity_col)\n.agg(pl.all().slice(max_lag))\n.explode(pl.all().exclude(entity_col))\n)\nartifacts = {\"X_new\": X_new}\nreturn artifacts\nreturn transform\n</code></pre> <p>Key points to note:</p> <ol> <li>Specify all parameters in the outer function.</li> <li>Implement a curried <code>transform</code> function inside the outer function that returns a dictionary. This dictionary must contain <code>X_new</code> key mapped to the transformed DataFrame. Every <code>transform</code> function expects a panel DataFrame.</li> </ol>"},{"location":"forecasting/","title":"Global Forecasting Walkthrough","text":""},{"location":"forecasting/#api-usage","title":"API Usage","text":"<p>All individual forecasters (e.g. <code>lasso</code> / <code>xgboost</code>) have the same API. Use <code>**kwargs</code> to pass custom hyperparameters into the underlying regressor (e.g. sklearn's <code>LinearRegression</code> regressor in functime's <code>linear_model</code> forecaster). Forecasters with automated hyperparameter tuning (e.g. <code>auto_lasso</code> and <code>auto_xgboost</code>) follow a similar API design. View API reference for details.</p>"},{"location":"forecasting/#supported-forecasters","title":"Supported Forecasters","text":"<p><code>functime</code> currently supports the following autoregressive global forecasters.</p> <p>Forecasters</p> <ul> <li><code>ann</code></li> <li><code>catboost</code></li> <li><code>censored_model</code></li> <li><code>elastic_net_cv</code></li> <li><code>elastic_net</code></li> <li><code>flaml_lightgbm</code></li> <li><code>knn</code></li> <li><code>lasso_cv</code></li> <li><code>lasso</code></li> <li><code>lightgbm</code></li> <li><code>linear_model</code></li> <li><code>ridge_cv</code></li> <li><code>ridge</code></li> <li><code>xgboost</code></li> <li><code>zero_inflated_model</code></li> </ul> <p>Automated Forecasters</p> <ul> <li><code>auto_elastic_net</code></li> <li><code>auto_knn</code></li> <li><code>auto_lasso</code></li> <li><code>auto_lightgbm</code></li> <li><code>auto_linear_model</code></li> <li><code>auto_ridge</code></li> </ul> <p><code>functime</code> also has the following benchmark models implemented as pure Polars queries.</p> <p>Benchmark Forecasters</p> <ul> <li><code>naive</code>: random walk forecaster</li> <li><code>snaive</code>: seasonal naive forecaster</li> </ul>"},{"location":"forecasting/#quickstart","title":"Quickstart","text":"<p>Want to go straight into code? Run through every forecasting example with the following script:</p> <code>quickstart.py</code> <pre><code>import json\nfrom timeit import default_timer\nimport polars as pl\nfrom functime.cross_validation import train_test_split\nfrom functime.feature_extraction import add_fourier_terms\nfrom functime.forecasting import auto_linear_model, linear_model, naive, snaive\nfrom functime.metrics import smape\nfrom functime.preprocessing import scale\nstart_time = default_timer()\n# Load data\ny = pl.read_parquet(\n\"https://github.com/descendant-ai/functime/raw/main/data/commodities.parquet\"\n)\nentity_col, time_col = y.columns[:2]\nX = y.select([entity_col, time_col]).pipe(add_fourier_terms(sp=12, K=6)).collect()\nprint(\"\ud83c\udfaf Target variable (y):\\n\", y)\nprint(\"\ud83d\udcc9 Exogenous variables (X):\\n\", X)\n# Train-test splits\ntest_size = 3\nfreq = \"1mo\"\ny_train, y_test = train_test_split(test_size)(y)\nX_train, X_test = train_test_split(test_size)(X)\n# Paralleized naive forecasts!\ny_pred_naive = naive(freq=\"1mo\")(y=y_train, fh=3)\ny_pred_snaive = snaive(freq=\"1mo\", sp=12)(y=y_train, fh=3)\n# Univariate time-series fit with automated lags and hyperparameter tuning\nauto_forecaster = auto_linear_model(\nfreq=freq, test_size=test_size, min_lags=12, max_lags=18, n_splits=3, time_budget=3\n)\nauto_forecaster.fit(y=y_train)\n# Predict\ny_pred = auto_forecaster.predict(fh=test_size)\n# Score\nscores = smape(y_true=y_test, y_pred=y_pred)\nprint(\"\u2705 Predictions (univariate):\\n\", y_pred.sort(entity_col))\nprint(\"\ud83d\udcaf Scores (univariate):\\n\", scores.sort(\"smape\"))\nprint(\"\ud83d\udcaf Scores summary (univariate):\\n\", scores.select(\"smape\").describe())\n# Retrieve best lags and hyperparameters\nbest_params = auto_forecaster.best_params\nprint(f\"\u2728 Best parameters (y only):\\n{json.dumps(best_params, indent=4)}\")\n# Multivariate\nforecaster = linear_model(**best_params)\nforecaster.fit(y=y_train, X=X_train)\n# Predict\ny_pred = forecaster.predict(fh=test_size, X=X_test)\n# Score\nscores_with_exog = smape(y_true=y_test, y_pred=y_pred)\nprint(\"\u2705 Predictions (multivariate):\\n\", y_pred.sort(entity_col))\nprint(\"\ud83d\udcaf Scores (multivariate):\\n\", scores_with_exog.sort(\"smape\"))\nprint(\"\ud83d\udcaf Scores summary (multivariate):\\n\", scores_with_exog.select(\"smape\").describe())\n# Check uplift from Fourier features\nuplift = (\nscores_with_exog.join(scores, on=entity_col, suffix=\"_univar\")\n.with_columns(\nuplift=pl.col(\"smape_univar\") - pl.col(\"smape\"),\nhas_uplift=pl.col(\"smape_univar\") - pl.col(\"smape\") &gt; 0,\n)\n.select([entity_col, \"uplift\", \"has_uplift\"])\n)\n# NOTE: Fourier features lead to uplift for ~20% of commodities\n# However, at the expense of an overall mean and variance SMAPE\n# (likely due to overfitting on seasonal features)\nprint(\"\ud83d\udcaf Uplift:\\n\", uplift.sort(\"uplift\", descending=True))\nprint(\"\ud83d\udcaf Proportion with uplift:\", uplift.get_column(\"has_uplift\").mean())\n# \"Direct\" strategy forecasting\nbest_params[\"max_horizons\"] = test_size  # Override max_horizons\nbest_params[\"strategy\"] = \"direct\"  # Override strategy\n# Predict using the \"functional\" API\ny_pred = linear_model(**best_params)(y=y_train, fh=test_size)\n# \"Ensemble\" strategy forecasting\nbest_params[\"strategy\"] = \"ensemble\"  # Override strategy\n# Backtesting\ny_preds = linear_model(**best_params).backtest(y=y_train, X=X_train)\nprint(\"\u2705 Backtests:\", y_preds)\n# Forecast with target transforms and feature transforms\nforecaster = linear_model(\nfreq=\"1mo\",\nlags=24,\ntarget_transform=scale(),\nfeature_transform=add_fourier_terms(sp=12, K=6),\n)\ny_pred = forecaster(y=y_train, fh=test_size)\nelapsed_time = default_timer() - start_time\nprint(f\"\u23f1\ufe0f Elapsed time: {elapsed_time}\")\n</code></pre>"},{"location":"forecasting/#prepare-data","title":"Prepare Data","text":"<p>Load a collection of time series, also known as panel data, into a <code>polars.LazyFrame</code> (recommended) or <code>polars.DataFrame</code> and split them into train/test subsets.</p> <pre><code>import polars as pl\nfrom functime.cross_validation import train_test_split\nfrom functime.metrics import mase\nfrom functime.feature_extraction import add_calendar_effects\n# Load data\ny = pl.read_parquet(\"https://github.com/descendant-ai/functime/raw/main/data/commodities.parquet\")\nentity_col, time_col = y.columns[:2]\nX = (\ny.select([entity_col, time_col])\n.pipe(add_calendar_effects([\"month\"]))\n.collect()\n)\n# Train-test splits\ntest_size = 3\nfreq = \"1mo\"\ny_train, y_test = train_test_split(test_size)(y)\nX_train, X_test = train_test_split(test_size)(X)\n</code></pre> <p>Supported Data Schemas</p> <p><code>X: polars.LazyFrame | polars.DataFrame</code> and <code>y: polars.LazyFrame | polars.DataFrame</code> must contain at least three columns. The first column must represent the <code>entity</code> / <code>series_id</code> dimension. The second column must represent the <code>time</code> dimension as an integer, <code>pl.Date</code>, or <code>pl.Datetime</code> series. Remaining columns are considered as features.</p>"},{"location":"forecasting/#fit-predict-score","title":"Fit / Predict / Score","text":"<p><code>functime</code> forecasters expose sklearn-compatible <code>.fit</code> and <code>.predict</code> methods. <code>functime.metrics</code> contains a comprehensive range of scoring functions for both point and probablistic forecasts.</p> Supported Forecast Metrics <pre><code>from functime.forecasting import linear_model\nfrom functime.metrics import mase\n# Fit\nforecaster = linear_model(lags=24, freq=\"1mo\")\nforecaster.fit(y=y_train)\n# Predict\ny_pred = forecaster.predict(fh=3)\n# Score\nscores = mase(y_true=y_test, y_pred=y_pred, y_train=y_train)\n</code></pre> <p>Supported Data Schemas</p> <p><code>X: polars.LazyFrame | polars.DataFrame</code> and <code>y: polars.LazyFrame | polars.DataFrame</code> must contain at least three columns. The first column must represent the <code>entity</code> / <code>series_id</code> dimension. The second column must represent the <code>time</code> dimension as an integer, <code>pl.Date</code>, or <code>pl.Datetime</code> series. Remaining columns are considered as features.</p> <p>functime \u2764\ufe0f currying</p> <p>Every <code>transformer</code> and <code>splitter</code> are curried functions.</p> <pre><code>from functime.preprocessing import boxcox, impute\n# Use df.pipe to chain operations together\nX_new: pl.LazyFrame = (\nX.pipe(boxcox(method=\"mle\"))\n.pipe(impute(method=\"linear\"))\n)\n# Call .collect to execute query\nX_new = X_splits.collect()\n</code></pre> <p>You can also use any <code>forecaster</code> as a curried function to run fit-predict in a single line of code.</p> <pre><code>from functime.forecasting import linear_model\ny_pred = linear_model(lags=24, freq=\"1mo\")(\ny=y_train,\nfh=28,\nX=X_train,\nX_future=X_test\n)\n</code></pre> <p>functime is lazy</p> <p><code>transformers</code> and <code>splitters</code> in <code>cross_validation</code>, <code>feature_extraction</code>, and <code>preprocessing</code> are lazy. These callables return <code>LazyFrames</code>, which represents a Lazy computation graph/query against the input <code>DataFrame</code> / <code>LazyFrame</code>. No computation is run until the <code>collect()</code> method is called on the <code>LazyFrame</code>.</p> <p><code>X</code> and <code>y</code> should be preprocessed lazily for optimal performance. Lazy evaluation allows <code>polars</code> to optimize all operations on the input <code>DataFrame</code> / <code>LazyFrame</code> at once. Lazy preprocessing in <code>functime</code> allows for more efficient <code>groupby</code> operations.</p> <p>With lazy transforms, operations series-by-series (e.g. <code>boxcox</code>, <code>impute</code>, <code>diff</code>) are chained in parallel: <code>groupby</code> is only called once. By contrast, with eager transforms, operations series-by-series is called in sequence: <code>groupby-aggregate</code> is called per transform.</p>"},{"location":"forecasting/#global-forecasting","title":"Global Forecasting","text":"<p>Every <code>forecaster</code> exposes a scikit-learn <code>fit</code> and <code>predict</code> API. The <code>fit</code> method takes <code>y</code> and <code>X</code> (optional). The <code>predict</code> method takes the forecast horizon <code>fh: int</code>, frequency alias <code>freq: str</code>, and <code>X</code> (optional).</p> <p>Supported Frequency Aliases</p> <ul> <li>1s (1 second)</li> <li>1m (1 minute)</li> <li>30m (30 minute)</li> <li>1h (1 hour)</li> <li>1d (1 day)</li> <li>1w (1 week)</li> <li>1mo (1 calendar month)</li> <li>3mo (1 calendar quarter)</li> <li>1y (1 calendar year)</li> <li>1i (1 index count)</li> </ul> <pre><code>from functime.forecasting import linear_model\nfrom functime.metrics import mase\n# Fit\nforecaster = linear_model(lags=24, freq=\"1mo\")\nforecaster.fit(y=y_train)\n# Predict\ny_pred = forecaster.predict(fh=3)\n# Score\nscores = mase(y_true=y_test, y_pred=y_pred, y_train=y_train)\n</code></pre> <p>Global vs Local Forecasting</p> <p><code>functime</code> only supports global forecasters. Global forecasters fit and predict a collection of time series using a single model. Local forecasters (e.g. ARIMA, ETS, Theta) fit and predict one series per model. Example collections of time series, which are also known as panel data, include:</p> <ul> <li>Sales across product in a retail store</li> <li>Churn rates across customer segments</li> <li>Sensor data across devices in a factory</li> <li>Delivery times across trucks in a logistics fleet</li> </ul> <p>Global forecasters, trained on a collection of similar time series, consistently outperform local forecasters.<sup>1</sup> Most notably, all top 50 competitors in the M5 Forecasting Competition used a global LightGBM forecasting model.<sup>2</sup></p> <p>Save 100x in Cloud spend</p> <p>Local forecasting is expensive and slow. To productionize forecasts at scale (&gt;1,000 series), local models have no choice but distributed computing. Every fit-predict call per local model per series are executed in parallel across the distributed cluster. Running a distributed cluster, however, is a significant cost and time sink for any data team.</p> <p><code>functime</code> believes that the vast majority of businesses do not need distributed computing to produce high-quality forecasts. Every <code>forecaster</code>, <code>transformer</code>, <code>splitter</code>, and <code>metric</code> in <code>functime</code> operates globally across collections of time series. We rewrote every time series operation in <code>polars</code> for blazing fast multi-threaded parallelism.</p> <p>If you are working at a reasonable-scale company, you most likely don't need Databricks to scale your forecasts. Use <code>functime</code>.</p>"},{"location":"forecasting/#benchmark-forecasters","title":"Benchmark Forecasters","text":"<p>Naive and seasonal naive forecasters are surprisingly hard to beat! You should always consider using the naive and seasonal naive forecasts as benchmarks. <code>functime</code> implements embarressingly parallel versions of the naive (random walk) and seasonal naive forecasters. These forecasters are expressed as pure Polars queries and executed in lazy streaming mode for speed and memory efficiency.</p> <pre><code>from functime.forecasting import naive, snaive\ny_pred_naive = naive(freq=\"1mo\")(y=y_train, fh=12)\n# sp = seasonal periods (length of one seasonal cycle)\ny_pred_snaive = naive(freq=\"1mo\", sp=12)(y=y_train, fh=12)\n</code></pre>"},{"location":"forecasting/#exogenous-regressors","title":"Exogenous Regressors","text":"<p>Every forecaster in <code>functime</code> supports exogenous regressors.</p> <pre><code>from functime.forecasting import linear_model\nforecaster = linear_model(lags=24, fit_intercept=False, freq=\"1mo\")\nforecaster.fit(y=y_train, X=X_train)\ny_pred = forecaster.predict(fh=3, X=X_test)\n</code></pre> <p>Naive Forecasting</p> <p>It is best practice to run naive forecasts (random walk, seasonal naive) as benchmarks. These simple forecasting methods can be remarkably difficult to beat!<sup>3</sup></p> <pre><code>from functime.forecasting import naive, snaive\n# Random walk model\ny_pred_naive = naive(freq=\"1mo\")(y=y_train, fh=3)\n# Seasonal naive model\ny_pred_snaive = snaive(freq=\"1mo\", sp=12)(y=y_train, fh=3)\n</code></pre>"},{"location":"forecasting/#transformations-preprocessing","title":"Transformations / Preprocessing","text":"<p>Every forecaster has two optional parameters <code>target_transform</code> and <code>feature_transform</code>, which take a <code>functime</code> transformer (e.g. <code>diff(order=1)</code>, <code>detrend(method=\"linear\")</code>).</p> <pre><code>- `target_transform` applies a transformation on `y` before fit and predict. An inverse transformation is then applied after predict to return the final forecast.\n- `feature_transform` applies a transformation on `X` before fit and predict.\n</code></pre> <p>We recommend using <code>target_transform</code> and <code>feature_transform</code> to avoid common pitfalls such as inconsistent feature engineering and data leakage. Check out the API reference for preprocessing and feature_extraction for a list of supported transformations.</p>"},{"location":"forecasting/#target-transform","title":"Target Transform","text":"<pre><code>from functime.forecasting import linear_model\nfrom functime.preprocessing import diff, scale, boxcox\n# Apply first differences\nforecaster = linear_model(freq=\"1mo\", lags=12, target_transform=diff(order=1))\n# Or local standarization\nforecaster = linear_model(freq=\"1mo\", lags=12, target_transform=scale())\n# Or Box-cox\nforecaster = linear_model(freq=\"1mo\", lags=12, target_transform=boxcox())\n</code></pre>"},{"location":"forecasting/#feature-transform","title":"Feature Transform","text":"<pre><code>from functime.forecasting import linear_model\nfrom functime.feature_extraction import add_fourier_terms\nfrom functime.preprocessing import lag\n# Include Fourier terms to model complex seasonality\nforecaster = linear_model(\nfreq=\"1mo\",\nlags=12,\nfeature_transform=add_fourier_terms(sp=12, K=3)\n)\n# Create lags of exogenous regressors\nforecaster = linear_model(\nfreq=\"1mo\",\nlags=12,\nfeature_transform=lag(lags=[1,2,3])\n)\n</code></pre>"},{"location":"forecasting/#target-and-feature-transform","title":"Target and Feature Transform","text":"<pre><code>from functime.forecasting import linear_model\nforecaster = linear_model(\nfreq=\"1mo\",\nlags=12,\ntarget_transform=scale(),\ntarget_transform=add_fourier_terms(sp=12, K=3)\n)\n</code></pre>"},{"location":"forecasting/#forecast-strategies","title":"Forecast Strategies","text":"<p><code>functime</code> supports three forecast strategies: <code>recursive</code>, <code>direct</code> multi-step, and a simple ensemble of both <code>recursive</code> and <code>direct</code>.</p> <p><pre><code>from functime.forecasting import linear_model\n# Recursive (Default)\nrecursive_forecaster = linear_model(strategy=\"recursive\")\ny_pred_rec = recursive_model(y_train, fh)\n# Direct\nmax_horizons = 12  # Number of direct models\ndirect_forecaster = = linear_model(strategy=\"direct\",max_horizons=max_horizons, freq=\"1mo\")\ny_pred_dir = recursive_model(y_train, fh)\n# Ensemble\nensemble_forecaster = linear_model(strategy=\"ensemble\", max_horizons=max_horizons, freq=\"1mo\")\ny_pred_ens = ensemble_model(y=y_train, fh=3)\n</code></pre> where <code>max_horizons</code> is the number of models specific to each forecast horizon. For example, if <code>max_horizons = 12</code>, then twelve forecasters are fitted in total: the 1-step ahead forecast, the 2-steps ahead forecast, the 3-steps ahead forecast, ..., and the final 12-steps ahead forecast.</p>"},{"location":"forecasting/#censored-forecasts","title":"Censored Forecasts","text":"<p>Most real-world datasets in e-commerce and logistics contain zeros in the target variable: e.g. periods with no sales. To address this problem, <code>functime</code> implements the <code>censored_model</code> forecaster, which trains a binary classifier and two forecasters. The binary classifier predicts the probability that a forecast falls above or below a certain threshold (e.g. zero). The final forecast is a weighted average of the above and below threshold forecasters.</p> <pre><code>from functime.forecasting import censored_model\n# Load the M5 competition Walmart dataset\ny_train = pl.read_parquet(\"data/m5_y_train.parquet\")\nX_train = pl.read_parquet(\"data/m5_X_train.parquet\")\n# Fit-predict given threshold = 0.0\ny_pred = censored_model(lags=3, threshold=0.0, freq=\"1d\")(\ny=y_train, X=X_train, fh=fh, X_future=X_test\n)\n</code></pre> <p>Custom Classfier and Regressors</p> <p>By default, <code>censored_model</code> uses sklearn's <code>HistGradientBoostingClassifier</code> and <code>HistGradientBoostingRegressor</code>. To use your own classifier and regressor, implement a function that takes <code>X</code> and <code>y</code> numpy arrays and returns a fitted sklearn-compatible classifier and regressor.</p> <pre><code>from sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestClassifier\ndef regress(X: np.ndarray, y: np.ndarray):\nregressor = MLPRegressor()\nregressor.fit(X=X, y=y)\nreturn regressor\ndef classify(X: np.ndarray, y: np.ndarray):\nclassifier = RandomForestClassifier()\nclassifier.fit(X=X, y=y))\nreturn classifier\n# Censored model with custom classifier and regressor\nforecaster = censored_model(\nlags=3,\nthreshold=0.0,\nfreq=\"1d\",\nclassify=classify,\nregress=regress\n)\ny_pred = forecaster(y=y_train, X=X_train, fh=fh, X_future=X_test)\n</code></pre>"},{"location":"forecasting/#automated-parameter-tuning","title":"Automated Parameter Tuning","text":"<p>Forecasters in auto_forecasting automatically tune the number of lagged regressors and the model's hyperparameters (e.g. <code>alpha</code> for <code>Lasso</code>). Cross-validation, lags tuning, and model parameters tuning are performed simultaneously for maximum efficiency.</p>"},{"location":"forecasting/#optimal-lag-length","title":"Optimal Lag Length","text":"<p><code>auto_{model}</code> forecasters automatically select the optimal number of lags via cross-validation. These forecasters conduct a search over possible models within <code>min_lags</code> and <code>max_lags</code>. The best model is the model with the lowest average RMSE (root mean squared error) across splits.</p> <pre><code>from functime.forecasting import auto_linear_model\n# Fit then predict\nforecaster = auto_linear_model(min_lags=20, max_lags=24, freq=\"1mo\")\nforecaster.fit(y=y_train, X=X_train)\ny_pred = forecaster.predict(fh=3, X=X_test)\n# Fit and predict\ny_pred = auto_linear_model(min_lags=20, max_lags=24, freq=\"1mo\")(\ny=y_train,\nX=X_train,\nX_future=X_test,\nfh=3\n)\n</code></pre>"},{"location":"forecasting/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p><code>auto_{model}</code> forecasters automatically select the optimal number of lags via cross-validation. These forecasters conduct a search over possible models within <code>min_lags</code> and <code>max_lags</code>. The best model is the model with the lowest average RMSE (root mean squared error) across splits.</p> <p>Sane Hyperparameter Defaults</p> <p>Sane defaults are used if <code>search_space</code> or <code>points_to_evaluate</code> are left as <code>None</code>. <code>functime</code> specify default hyperparameters search spaces according to best-practices from industry, top Kaggle solutions, and research.</p> <p><code>functime</code> uses <code>FLAML</code> under the hood to conduct hyperparameter tuning.</p> <pre><code>from flaml import tune\nfrom functime.forecasting import auto_lightgbm\n# Specify search space, initial conditions, and time budget\nsearch_space = {\n\"reg_alpha\": tune.loguniform(1e-08, 10.0),\n\"reg_lambda\": tune.loguniform(1e-08, 10.0),\n\"num_leaves\": tune.randint(\n2, 2**max_depth if max_depth &gt; 0 else 2**DEFAULT_TREE_DEPTH\n),\n\"colsample_bytree\": tune.uniform(0.4, 1.0),\n\"subsample\": tune.uniform(0.4, 1.0),\n\"subsample_freq\": tune.randint(1, 7),\n\"min_child_samples\": tune.qlograndint(5, 100, 5),\n}\npoints_to_evaluate = [\n{\n\"num_leaves\": 31,\n\"colsample_bytree\": 1.0,\n\"subsample\": 1.0,\n\"min_child_samples\": 20,\n}\n]\ntime_budget = 420\n# Fit model\nforecaster = auto_lightgbm(\nfreq=\"1mo',\nmin_lags=20,\nmax_lags=24,\ntime_budget=time_budget,\nsearch_space=search_space,\npoints_to_evaluate=points_to_evaluate\n)\nforecaster.fit(y=y_train)\n# Get best lags and model hyperparameters\nbest_params = forecaster.best_params\n</code></pre>"},{"location":"forecasting/#backtesting","title":"Backtesting","text":"<p>Every <code>forecaster</code> and <code>auto_forecaster</code> has a <code>backtest</code> method. <code>functime</code> supports both <code>expanding_window_split</code> and <code>sliding_window_split</code> for backtesting and cross-validation.</p> <pre><code>from functime.forecasting import linear_model\nforecaster = linear_model(lags=24, fit_intercept=False, freq=\"1mo\")\ny_preds, y_resids = forecaster.backtest(\ny=y_train,\nX=X_train,\ntest_size=6,\nstep_size=1,\nn_splits=3,\nwindow_size=1,  # Only applicable if `strategy` equals \"sliding\"\nstrategy=\"expanding\",\n# Raises ValueError if drop_short=False and there are\n# entities with insufficient length for cross-validation\ndrop_short=True\n)\n</code></pre>"},{"location":"forecasting/#probablistic-forecasts","title":"Probablistic Forecasts","text":"<p><code>functime</code> supports two methods for generating prediction intervals.</p>"},{"location":"forecasting/#quantile-regression","title":"Quantile Regression","text":"<p>Supported by <code>LightGBM</code>, <code>XGBoost</code>, and <code>Catboost</code> forecasters and their automated equivalents.</p> <pre><code>from functime.forecasting import auto_lightgbm\n# Forecasts at 10th and 90th percentile\ny_pred_10 = auto_lightgbm(alpha=0.1, freq=\"1d\")(y=y_train, fh=28)\ny_pred_90 = auto_lightgbm(alpha=0.9, freq=\"1d\")(y=y_train, fh=28)\n</code></pre>"},{"location":"forecasting/#conformal-prediction","title":"Conformal Prediction","text":"<p><code>functime</code> currently supports batch prediction intervals (EnbPI) from the paper Conformal prediction interval for dynamic time-series</p> <pre><code>from functime.conformal import conformalize\nfrom functime.forecasting import linear_model\nforecaster = linear_model(lags=24, fit_intercept=False, freq=\"1mo\")\ny_preds, y_resids = forecaster.backtest(y=y_train, X=X_train)\n# Forecasts at 10th and 90th percentile\n# Requires forecast (y_pred), backtest values (y_preds),\n# and residuals from backtest (y_resid)\ny_pred_quantiles = conformalize(\ny_pred=y_pred,\ny_preds=y_preds,\ny_resids=y_resids,\nalphas=[0.1, 0.9]\n)\n</code></pre> <ol> <li> <p>Montero-Manso, P., &amp; Hyndman, R. J. (2021). Principles and algorithms for forecasting groups of time series: Locality and globality. International Journal of Forecasting, 37(4), 1632-1653.\u00a0\u21a9</p> </li> <li> <p>Makridakis, S., Spiliotis, E., &amp; Assimakopoulos, V. (2022). M5 accuracy competition: Results, findings, and conclusions. International Journal of Forecasting.\u00a0\u21a9</p> </li> <li> <p>https://otexts.com/fpp3/simple-methods.html\u00a0\u21a9</p> </li> </ol>"},{"location":"installation/","title":"Installation","text":"<p>functime is published as Python package in PyPI. To install the latest functime release, run the following command:</p> <pre><code>pip install functime\n</code></pre>"},{"location":"installation/#extras","title":"Extras","text":"<p><code>functime</code> comes with extra options. For example, to install <code>functime</code> with large-language model (LLM) and lightgbm features:</p> <pre><code>pip install \"functime[llm,lightgbm]\"\n</code></pre> <ul> <li><code>cat</code>: To use <code>catboost</code> forecaster</li> <li><code>xgb</code>: To use <code>xgboost</code> forecaster</li> <li><code>lgb</code>: To use <code>lightgbm</code> forecaster</li> <li><code>llm</code>: To use the LLM-powered forecast analyst</li> </ul>"},{"location":"preprocessing/","title":"Preprocessing","text":"<p><code>functime</code> supports parallelized time-series preprocessing using Polars. All <code>functime</code> preprocessors take a panel DataFrame as a input and transform each time-series locally (i.e. time-series by time-series as a parallelized groupby operation).</p> <p>Time-series transformations are commonly used to stabilize the time-series (e.g. <code>boxcox</code> for variance stabilzation) or make the time-series stationary through first differences or detrending. Some transformations are also invertible, such as <code>diff</code> and <code>detrend</code>, which is useful for converting the forecast of a transformed time-series back to the original scale.</p> <p>Check out the API reference for details.</p>"},{"location":"preprocessing/#quick-examples","title":"Quick Examples","text":""},{"location":"preprocessing/#differencing","title":"Differencing","text":"<p>Apply k-order differences. This transform is invertible.</p> <pre><code>from functime.preprocessing import diff\ntransformer = diff(order=1)\nX_new = X.pipe(transformer).collect()\nX_original = transformer.invert(X_new)\n</code></pre>"},{"location":"preprocessing/#seasonal-differencing","title":"Seasonal Differencing","text":"<p>Apply k-order differences shifted by <code>sp</code> periods. This transform is invertible.</p> <pre><code>from functime.preprocessing import diff\n# Assume X is a monthly dataset with seasonal period = 12\ntransformer = diff(order=1, sp=12)\nX_new = X.pipe(transformer).collect()\nX_original = transformer.invert(X_new)\n</code></pre>"},{"location":"preprocessing/#detrending-linear","title":"Detrending (Linear)","text":"<p>Removes linear trend for each time-series. This transform is invertible.</p> <pre><code>from functime.preprocessing import detrend\ntransformer = detrend(method=\"linear\")\nX_new = X.pipe(transformer).collect()\nX_original = transformer.invert(X_new)\n</code></pre>"},{"location":"preprocessing/#detrending-mean","title":"Detrending (Mean)","text":"<p>Removes mean trend for each time-series. This transform is invertible.</p> <pre><code>from functime.preprocessing import detrend\ntransformer = detrend(method=\"mean\")\nX_new = X.pipe(transformer).collect()\nX_original = transformer.invert(X_new)\n</code></pre>"},{"location":"preprocessing/#box-cox","title":"Box-Cox","text":"<p>Applies optimized Box-Cox transform for each time-series. This transform is invertible.</p> <pre><code>from functime.preprocessing import boxcox\ntransformer = boxcox(method=\"mle\")\nX_new = X.pipe(transformer).collect()\nX_original = transformer.invert(X_new)\n</code></pre>"},{"location":"preprocessing/#local-scaling","title":"Local Scaling","text":"<p>Standardizes each time-series with subtracting mean and dividing by the standard deviation. This transform is invertible.</p> <pre><code>from functime.preprocessing import scale\ntransformer = scale(use_mean=True, use_std=True)\nX_new = X.pipe(transformer).collect()\nX_original = transformer.invert(X_new)\n</code></pre>"},{"location":"preprocessing/#rolling-statistics","title":"Rolling Statistics","text":"<p>Given a list of window sizes, applies rolling statistics for each time-series across each column. This transform is not invertible. Currently supports the following statistics: <code>mean</code>, <code>min</code>, <code>max</code>, <code>mlm</code> (max less min), <code>sum</code>, <code>std</code>, <code>cv</code> (coefficient of variation).</p> <pre><code>from functime.preprocessing import roll\n# The following code generates moving averages (MA10, MA30 and MA60)\n# and moving sums for a panel dataset of daily time-series.\ntransformer = roll(\nwindow_sizes=[10, 30, 60],\nstats=[\"mean\", \"sum\"],\nfreq=\"1d\"\n)\nX_new = X.pipe(transformer).collect()\n</code></pre>"},{"location":"seasonality/","title":"Seasonality","text":""},{"location":"seasonality/#seasonality-and-holiday-effects","title":"Seasonality and Holiday Effects","text":""},{"location":"seasonality/#modelling-seasonality","title":"Modelling Seasonality","text":""},{"location":"seasonality/#seasonal-periods","title":"Seasonal Periods","text":"<p>Given a Polars offset alias <code>freq</code>, use <code>functime.offsets.freq_to_sp</code> to return a list of seasonal periods.</p> <pre><code>seasonal_periods = {\n\"1s\": [60, 3_600, 86_400, 604_800, 31_557_600],\n\"1m\": [60, 1_440, 10_080, 525_960],\n\"30m\": [48, 336, 17_532],\n\"1h\": [24, 168, 8_766],\n\"1d\": [7, 365],\n\"1w\": [52],\n\"1mo\": [12],\n\"3mo\": [4],\n\"1y\": [1],\n}\n</code></pre>"},{"location":"seasonality/#method-1-dummy-variables-categorical","title":"Method 1. Dummy Variables / Categorical","text":"<p>Use <code>add_calendar_effects</code> to generate datetime and calendar effects. <code>functime</code> supports two strategies to model seasonality as discrete features: though a categorical column (useful for forecasters with native categorical features support e.g. <code>lightgbm</code>) or multiple binary columns (i.e. one-hot encoding). Check out Chapter 7.4: Seasonal dummy variables for a quick primer.</p> <p>If you choose the dummy variable strategy, beware of the \"dummy variable trap\" (i.e. remember to set <code>fit_intercept=False</code> if you decide to include all dummy columns).</p> <ul> <li>minute: 1, 2, ..., 60 (in a day)</li> <li>hour: 1, 2, ..., 24 (in a day)</li> <li>day: 1, 2, ..., 31 (in a month)</li> <li>weekday: 1, 2, ..., 7 (in a week)</li> <li>week: 1, 2,..., 52 (in a year)</li> <li>quarter: 1, 2, ..., 4 (in a year)</li> <li>year: 1999, 2000, ..., 2023 (any year)</li> </ul> <pre><code>from functime.feature_extraction import add_calendar_effects\n# Returns X with one categorical column \"month\" with values 1,2,...,12\nX_new = X.pipe(add_calendar_effects([\"month\"])).collect()\n# Returns X with one-hot encoded calendar effects\n# i.e. binary columns \"month_1\", \"month_2\", ..., \"month_12\"\nX_new = X.pipe(add_calendar_effects([\"month\"]), as_dummies=True).collect()\n</code></pre>"},{"location":"seasonality/#method-2-fourier-terms","title":"Method 2. Fourier Terms","text":"<p>Fourier terms are a common way to model multiple seasonal periods and complex seasonality (e.g. long seasonal periods 365.25 / 7 \u2248 52.179 for weekly time series). For every seasonal period <code>sp</code> and Fourier term <code>k=1,..,K</code> pair, there are 2 fourier terms <code>sin_sp_k</code> and <code>cos_sp_k</code>.</p> <p>Fourier terms can be used to approximate a continuous periodic signal, which can then be used as exogenous regressors to model seasonality. Chapter 12.1: Complex Seasonality from Hyndman's textbook \"Forecasting: Principles and Practice\" contains a great practical introduction to this topic.</p> <p><code>add_fourier_terms</code> returns the original <code>X</code> DataFrame along with the Fourier terms as additional columns. For example, if <code>sp=12</code> and <code>K=3</code>, <code>X_new</code> would contain the columns <code>sin_12_1</code>, <code>cos_12_1</code>, <code>sin_12_2</code>, <code>cos_12_2</code>, <code>sin_12_3</code>, and <code>cos_12_3</code>.</p> <pre><code>from functime.offsets import freq_to_sp\nfrom functime.feature_extraction import add_fourier_terms\nsp = freq_to_sp[\"1mo\"][0]\nX_new = X.pipe(add_fourier_terms(sp=sp, K=3)).collect()\n</code></pre>"},{"location":"seasonality/#modelling-holidays-special-events","title":"Modelling Holidays / Special Events","text":"<p><code>functime</code> has a wrapper function around the <code>holidays</code> Python package to generate categorical features for special events. Dates without a holiday are filled with nulls.</p> <pre><code>from functime.feature_extraction import add_holiday_effects\n# Returns X with two categorical columns \"holiday__US\" and \"holiday__CA\"\nnorth_america_holidays = add_holiday_effects(country_codes=[\"US\", \"CA\"])\nX_new = X.pipe(north_america_holidays).collect()\n# Returns X with one-hot encoded holidays (e.g. \"holiday__US_christmas)\nnorth_america_holidays = add_holiday_effects(country_codes=[\"US\", \"CA\"], as_dummies=True)\nX_new = X.pipe(north_america_holidays).collect()\n</code></pre> <p>Custom Events</p> <p>If you have your own custom special events (e.g. special promotions), you can always create your own dummy variables as Polars boolean series.</p>"},{"location":"benchmarks/benckmarks/","title":"Global Forecasting Benchmarks","text":""},{"location":"notebooks/evaluation/","title":"Evaluation Procedure","text":"<p>Let's score, rank, and plot point forecasts for multiple commodity prices. We compare two forecasting models:</p> <ul> <li>Seasonal naive (as our benchmark)</li> <li>AR (autoregressive) LightGBM model with local scaling</li> </ul> In\u00a0[1]: Copied! <pre>%%capture\nimport polars as pl\n\nfrom functime.cross_validation import train_test_split\nfrom functime.evaluation import rank_point_forecasts, rank_fva, rank_residuals\nfrom functime.forecasting import lightgbm, snaive\nfrom functime.preprocessing import detrend\nfrom functime.plotting import plot_comet, plot_forecasts, plot_fva, plot_residuals\n</pre> %%capture import polars as pl  from functime.cross_validation import train_test_split from functime.evaluation import rank_point_forecasts, rank_fva, rank_residuals from functime.forecasting import lightgbm, snaive from functime.preprocessing import detrend from functime.plotting import plot_comet, plot_forecasts, plot_fva, plot_residuals In\u00a0[2]: Copied! <pre>fh = 12\ny = pl.read_parquet(\"../../data/commodities.parquet\")\nentity_col = y.columns[0]\ny_train, y_test = train_test_split(test_size=fh, eager=True)(y)\ny.tail()\n</pre> fh = 12 y = pl.read_parquet(\"../../data/commodities.parquet\") entity_col = y.columns[0] y_train, y_test = train_test_split(test_size=fh, eager=True)(y) y.tail() Out[2]: shape: (5, 3)commodity_typetimepricestrdatetime[ns]f64\"Zinc\"2022-11-01 00:00:002938.92\"Zinc\"2022-12-01 00:00:003129.48\"Zinc\"2023-01-01 00:00:003309.81\"Zinc\"2023-02-01 00:00:003133.84\"Zinc\"2023-03-01 00:00:002967.46 <p>There are 71 unique commoditity types and 759 dates (monthly) between and 1960-01-01 and 2023-03-01.</p> In\u00a0[3]: Copied! <pre>y.select(\n    pl.all().exclude(\"price\").n_unique(),\n    pl.col(\"time\").min().dt.date().alias(\"start\"),\n    pl.col(\"time\").max().dt.date().alias(\"end\")\n)\n</pre> y.select(     pl.all().exclude(\"price\").n_unique(),     pl.col(\"time\").min().dt.date().alias(\"start\"),     pl.col(\"time\").max().dt.date().alias(\"end\") ) Out[3]: shape: (1, 4)commodity_typetimestartendu32u32datedate717591960-01-012023-03-01 In\u00a0[4]: Copied! <pre>y_pred_bench = snaive(freq=\"1mo\", sp=12)(y=y_train, fh=fh)\ny_pred_bench.head()\n</pre> y_pred_bench = snaive(freq=\"1mo\", sp=12)(y=y_train, fh=fh) y_pred_bench.head() Out[4]: shape: (5, 3)commodity_typetimepricestrdatetime[\u03bcs]f64\"Rubber, RSS3\"2022-04-01 00:00:002.15\"Rubber, RSS3\"2022-05-01 00:00:002.29\"Rubber, RSS3\"2022-06-01 00:00:002.12\"Rubber, RSS3\"2022-07-01 00:00:001.87\"Rubber, RSS3\"2022-08-01 00:00:001.9 <p>Let's plot the top 4 forecasts by best SMAPE score:</p> In\u00a0[5]: Copied! <pre>ranks = rank_point_forecasts(\n    y_true=y_test,\n    y_pred=y_pred_bench\n)\nranks.head()\n</pre> ranks = rank_point_forecasts(     y_true=y_test,     y_pred=y_pred_bench ) ranks.head() Out[5]: shape: (5, 2)commodity_typesmapestrf64\"Gold\"0.016228\"Tobacco, US im\u20260.017253\"Sugar, US\"0.02763\"Groundnut oil \u20260.027752\"Cocoa\"0.027986 In\u00a0[6]: Copied! <pre>selected_entities = ranks.head(4).get_column(entity_col).unique()\nfigure = plot_forecasts(\n    y_true=y.filter(pl.col(entity_col).is_in(selected_entities)),\n    y_pred=y_pred_bench.filter(pl.col(entity_col).is_in(selected_entities)),\n    n_cols=2,\n    height=1000,\n    width=1200\n)\nfigure.show(renderer=\"svg\")\n</pre> selected_entities = ranks.head(4).get_column(entity_col).unique() figure = plot_forecasts(     y_true=y.filter(pl.col(entity_col).is_in(selected_entities)),     y_pred=y_pred_bench.filter(pl.col(entity_col).is_in(selected_entities)),     n_cols=2,     height=1000,     width=1200 ) figure.show(renderer=\"svg\") <p>Let's plot the top 3 forecasts by worst SMAPE score:</p> In\u00a0[7]: Copied! <pre>ranks = rank_point_forecasts(\n    y_true=y_test,\n    y_pred=y_pred_bench,\n    descending=True\n)\nranks.head()\n</pre> ranks = rank_point_forecasts(     y_true=y_test,     y_pred=y_pred_bench,     descending=True ) ranks.head() Out[7]: shape: (5, 2)commodity_typesmapestrf64\"Natural gas, E\u20260.372189\"Coal, Australi\u20260.365411\"Phosphate rock\u20260.354421\"Natural gas in\u20260.328693\"Coal, South Af\u20260.28314 In\u00a0[8]: Copied! <pre>selected_entities = ranks.head(4).get_column(entity_col).unique()\nfigure = plot_forecasts(\n    y_true=y.filter(pl.col(entity_col).is_in(selected_entities)),\n    y_pred=y_pred_bench.filter(pl.col(entity_col).is_in(selected_entities)),\n    n_cols=2,\n    height=1000,\n    width=1200\n)\nfigure.show(renderer=\"svg\")\n</pre> selected_entities = ranks.head(4).get_column(entity_col).unique() figure = plot_forecasts(     y_true=y.filter(pl.col(entity_col).is_in(selected_entities)),     y_pred=y_pred_bench.filter(pl.col(entity_col).is_in(selected_entities)),     n_cols=2,     height=1000,     width=1200 ) figure.show(renderer=\"svg\") In\u00a0[9]: Copied! <pre># Instantiate forecaster and backtest\nforecaster = lightgbm(freq=\"1mo\", lags=48, target_transform=detrend())\ny_preds, y_resids = forecaster.backtest(\n    y=y,\n    test_size=12,\n    step_size=12,\n    n_splits=5\n)\ny_pred = forecaster(fh=fh, y=y_train)\n</pre> # Instantiate forecaster and backtest forecaster = lightgbm(freq=\"1mo\", lags=48, target_transform=detrend()) y_preds, y_resids = forecaster.backtest(     y=y,     test_size=12,     step_size=12,     n_splits=5 ) y_pred = forecaster(fh=fh, y=y_train) <p>Let's plot backtests for commodities with the best SMAPE score across expanding window splits:</p> In\u00a0[10]: Copied! <pre>ranks = rank_point_forecasts(\n    y_true=y,\n    y_pred=y_preds,\n)\nranks.head()\n</pre> ranks = rank_point_forecasts(     y_true=y,     y_pred=y_preds, ) ranks.head() Out[10]: shape: (5, 2)commodity_typesmapestrf64\"Lead\"0.047562\"Rice, Thai 25%\u20260.049113\"Tea, avg 3 auc\u20260.049359\"Plywood\"0.050659\"Logs, Malaysia\u20260.053753 In\u00a0[11]: Copied! <pre>selected_entities = ranks.head(4).get_column(entity_col).unique()\nfigure = plot_forecasts(\n    y_true=y.filter(pl.col(entity_col).is_in(selected_entities)),\n    y_pred=y_preds.filter(pl.col(entity_col).is_in(selected_entities)),\n    n_cols=2,\n    height=1000,\n    width=1200\n)\nfigure.show(renderer=\"svg\")\n</pre> selected_entities = ranks.head(4).get_column(entity_col).unique() figure = plot_forecasts(     y_true=y.filter(pl.col(entity_col).is_in(selected_entities)),     y_pred=y_preds.filter(pl.col(entity_col).is_in(selected_entities)),     n_cols=2,     height=1000,     width=1200 ) figure.show(renderer=\"svg\") <p>Let's plot backtests for commodities with the worst SMAPE score across expanding window splits:</p> In\u00a0[12]: Copied! <pre>ranks = rank_point_forecasts(\n    y_true=y,\n    y_pred=y_preds,\n    descending=True\n)\nranks.head()\n</pre> ranks = rank_point_forecasts(     y_true=y,     y_pred=y_preds,     descending=True ) ranks.head() Out[12]: shape: (5, 2)commodity_typesmapestrf64\"Natural gas, E\u20260.453647\"Potassium chlo\u20260.331863\"Natural gas in\u20260.31516\"Coal, Australi\u20260.306185\"Phosphate rock\u20260.289096 In\u00a0[13]: Copied! <pre>selected_entities = ranks.head(4).get_column(entity_col).unique()\nfigure = plot_forecasts(\n    y_true=y.filter(pl.col(entity_col).is_in(selected_entities)),\n    y_pred=y_preds.filter(pl.col(entity_col).is_in(selected_entities)),\n    n_cols=2,\n    height=1000,\n    width=1200\n)\nfigure.show(renderer=\"svg\")\n</pre> selected_entities = ranks.head(4).get_column(entity_col).unique() figure = plot_forecasts(     y_true=y.filter(pl.col(entity_col).is_in(selected_entities)),     y_pred=y_preds.filter(pl.col(entity_col).is_in(selected_entities)),     n_cols=2,     height=1000,     width=1200 ) figure.show(renderer=\"svg\") <p>Let's plot residuals for top 10 forecasts ranked by highest mean absolute bias:</p> In\u00a0[14]: Copied! <pre>ranks = rank_residuals(\n    y_resids=y_resids,\n    sort_by=\"abs_bias\",\n    descending=True\n)\nranks.head()\n</pre> ranks = rank_residuals(     y_resids=y_resids,     sort_by=\"abs_bias\",     descending=True ) ranks.head() Out[14]: shape: (5, 2)commodity_typeabs_biasstrf64\"Tin\"3647.285338\"Nickel\"2888.592952\"Zinc\"144.139823\"Fish meal\"104.568748\"Tobacco, US im\u2026101.051888 In\u00a0[15]: Copied! <pre>selected_entities = ranks.head(4).get_column(entity_col).unique()\nfigure = plot_residuals(\n    y_resids=y_resids.filter(pl.col(entity_col).is_in(selected_entities)),\n    n_bins=200,\n    height=800,\n    width=1000\n)\nfigure.show(renderer=\"svg\")\n</pre> selected_entities = ranks.head(4).get_column(entity_col).unique() figure = plot_residuals(     y_resids=y_resids.filter(pl.col(entity_col).is_in(selected_entities)),     n_bins=200,     height=800,     width=1000 ) figure.show(renderer=\"svg\") <p>Let's plot residuals for top 10 forecasts ranked by normality test with the null hypothesis that the residuals are normally distributed. Higher test statistic = more likely to reject null hypothesis under the assumption that the null is true.</p> In\u00a0[16]: Copied! <pre>ranks = rank_residuals(\n    y_resids=y_resids,\n    sort_by=\"normality\",\n    descending=True\n)\nranks.head()\n</pre> ranks = rank_residuals(     y_resids=y_resids,     sort_by=\"normality\",     descending=True ) ranks.head() Out[16]: shape: (5, 2)commodity_typenormal_teststrf64\"Natural gas, E\u20264321.977586\"Phosphate rock\u20262351.334475\"Potassium chlo\u20262195.88077\"Coal, Australi\u20261917.223012\"TSP\"1756.461335 In\u00a0[17]: Copied! <pre>selected_entities = ranks.head(4).get_column(entity_col).unique()\nfigure = plot_residuals(\n    y_resids=y_resids.filter(pl.col(entity_col).is_in(selected_entities)),\n    n_bins=200,\n    height=800,\n    width=1000\n)\nfigure.show(renderer=\"svg\")\n</pre> selected_entities = ranks.head(4).get_column(entity_col).unique() figure = plot_residuals(     y_resids=y_resids.filter(pl.col(entity_col).is_in(selected_entities)),     n_bins=200,     height=800,     width=1000 ) figure.show(renderer=\"svg\") In\u00a0[18]: Copied! <pre>figure = plot_fva(\n    y_true=y_test,\n    y_pred=y_pred,\n    y_pred_bench=y_pred_bench,\n    height=900,\n    width=900\n)\nfigure.show(renderer=\"svg\")\n</pre> figure = plot_fva(     y_true=y_test,     y_pred=y_pred,     y_pred_bench=y_pred_bench,     height=900,     width=900 ) figure.show(renderer=\"svg\") In\u00a0[19]: Copied! <pre>figure = plot_comet(\n    y_train=y_train,\n    y_test=y_test,\n    y_pred=y_pred,\n    height=900,\n    width=900\n)\nfigure.show(renderer=\"svg\")\n</pre> figure = plot_comet(     y_train=y_train,     y_test=y_test,     y_pred=y_pred,     height=900,     width=900 ) figure.show(renderer=\"svg\")"},{"location":"notebooks/evaluation/#evaluation-procedure","title":"Evaluation Procedure\u00b6","text":"<p>This walkthrough covers the <code>functime.evaluation</code> module, which contains functions to rank forecasts and time-series features (e.g. tsfresh).</p>"},{"location":"notebooks/evaluation/#forecasting","title":"Forecasting\u00b6","text":""},{"location":"notebooks/evaluation/#load-data","title":"Load data\u00b6","text":""},{"location":"notebooks/evaluation/#benchmark-forecasts","title":"Benchmark Forecasts\u00b6","text":""},{"location":"notebooks/evaluation/#global-ar-forecasts","title":"Global AR Forecasts\u00b6","text":""},{"location":"notebooks/evaluation/#fva-forecast-value-add-plot","title":"FVA (Forecast Value Add) Plot\u00b6","text":"<p>It is best-practice to have use a single forecast model as a \"benchmark\". In this walkthrough, we compare seasonal naive forecast to our more \"sophisticated\" autoregressive LightGBM model.</p>"},{"location":"notebooks/evaluation/#comet-plot","title":"Comet Plot\u00b6","text":"<p>Plot of coefficient of variance (CV) against forecast accuracy (SMAPE by default). CV is the ratio of a pattern\u2019s standard deviation to its mean. It expresses the variability of a pattern over time. A flat line would have CV = 0. A highly erratic pattern may have CV of 100% or more. It can be an intuitive heuristic to determine the forecastability across many time-series!</p>"},{"location":"notebooks/evaluation/#feature-engineering","title":"Feature Engineering\u00b6","text":"<p>Coming soon.</p>"},{"location":"notebooks/evaluation/#fforma-feature-based-model-selection","title":"FFORMA (Feature-based Model Selection)\u00b6","text":"<p>Coming soon.</p>"},{"location":"notebooks/llm/","title":"LLM Interoperability","text":"<p>Let's use OpenAI's GPT models to analyze commodity price forecasts created by a functime forecaster. By default we use <code>gpt-3.5-turbo</code>.</p> In\u00a0[9]: Copied! <pre>%%capture\nimport os\nimport polars as pl\n\nfrom functime.cross_validation import train_test_split\nfrom functime.forecasting import knn\n\nimport functime.llm     # We must import this to override the `llm` namespace for pl.DataFrame\nfrom functime.llm.formatting import univariate_panel_to_wide\n</pre> %%capture import os import polars as pl  from functime.cross_validation import train_test_split from functime.forecasting import knn  import functime.llm     # We must import this to override the `llm` namespace for pl.DataFrame from functime.llm.formatting import univariate_panel_to_wide  In\u00a0[11]: Copied! <pre>os.environ[\"OPENAI_API_KEY\"] = ...  # Your API key here\n</pre> os.environ[\"OPENAI_API_KEY\"] = ...  # Your API key here In\u00a0[12]: Copied! <pre>y = pl.read_parquet(\"../../data/commodities.parquet\")\nentity_col, time_col, target_col = y.columns\ntest_size = 30\nfreq = \"1mo\"\ny_train, y_test = train_test_split(test_size)(y)\nprint(\"\ud83c\udfaf Target variable (y) -- train set:\\n\", y_train.collect())\n</pre> y = pl.read_parquet(\"../../data/commodities.parquet\") entity_col, time_col, target_col = y.columns test_size = 30 freq = \"1mo\" y_train, y_test = train_test_split(test_size)(y) print(\"\ud83c\udfaf Target variable (y) -- train set:\\n\", y_train.collect()) <pre>\ud83c\udfaf Target variable (y) -- train set:\n shape: (45_453, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 commodity_type    \u2506 time                \u2506 price \u2502\n\u2502 ---               \u2506 ---                 \u2506 ---   \u2502\n\u2502 str               \u2506 datetime[ns]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Coal, Australian  \u2506 1970-01-01 00:00:00 \u2506 7.8   \u2502\n\u2502 Coal, Australian  \u2506 1970-02-01 00:00:00 \u2506 7.8   \u2502\n\u2502 Coal, Australian  \u2506 1970-03-01 00:00:00 \u2506 7.8   \u2502\n\u2502 Coal, Australian  \u2506 1970-04-01 00:00:00 \u2506 7.8   \u2502\n\u2502 \u2026                 \u2506 \u2026                   \u2506 \u2026     \u2502\n\u2502 Natural gas index \u2506 2020-06-01 00:00:00 \u2506 33.99 \u2502\n\u2502 Natural gas index \u2506 2020-07-01 00:00:00 \u2506 34.91 \u2502\n\u2502 Natural gas index \u2506 2020-08-01 00:00:00 \u2506 45.85 \u2502\n\u2502 Natural gas index \u2506 2020-09-01 00:00:00 \u2506 46.07 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <p>We'll make a prediction using a knn forecaster.</p> In\u00a0[13]: Copied! <pre># Univariate time-series fit with automated lags\nforecaster = knn(\n    freq=\"1mo\",\n    lags=24\n)\nforecaster.fit(y=y_train)\ny_pred = forecaster.predict(fh=test_size)\nprint(\"\ud83d\udcca Preds:\\n\", y_pred)\n</pre> # Univariate time-series fit with automated lags forecaster = knn(     freq=\"1mo\",     lags=24 ) forecaster.fit(y=y_train) y_pred = forecaster.predict(fh=test_size) print(\"\ud83d\udcca Preds:\\n\", y_pred) <pre>\ud83d\udcca Preds:\n shape: (2_130, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 commodity_type          \u2506 time                \u2506 price       \u2502\n\u2502 ---                     \u2506 ---                 \u2506 ---         \u2502\n\u2502 str                     \u2506 datetime[\u03bcs]        \u2506 f64         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Tobacco, US import u.v. \u2506 2020-10-01 00:00:00 \u2506 4350.390137 \u2502\n\u2502 Tobacco, US import u.v. \u2506 2020-11-01 00:00:00 \u2506 4350.390137 \u2502\n\u2502 Tobacco, US import u.v. \u2506 2020-12-01 00:00:00 \u2506 4350.390137 \u2502\n\u2502 Tobacco, US import u.v. \u2506 2021-01-01 00:00:00 \u2506 4340.333984 \u2502\n\u2502 \u2026                       \u2506 \u2026                   \u2506 \u2026           \u2502\n\u2502 Sawnwood, Cameroon      \u2506 2022-12-01 00:00:00 \u2506 534.277954  \u2502\n\u2502 Sawnwood, Cameroon      \u2506 2023-01-01 00:00:00 \u2506 529.589966  \u2502\n\u2502 Sawnwood, Cameroon      \u2506 2023-02-01 00:00:00 \u2506 523.410034  \u2502\n\u2502 Sawnwood, Cameroon      \u2506 2023-03-01 00:00:00 \u2506 510.354004  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <p>We'll also provide a short description of the dataset to aid the LLM in its analysis.</p> In\u00a0[14]: Copied! <pre>dataset_context = \"This dataset comprises of forecasted commodity prices between 2020 to 2023.\"\n</pre> dataset_context = \"This dataset comprises of forecasted commodity prices between 2020 to 2023.\" <p>Let's take a look at aluminum and European banana prices. We'll first transform the panel dataframe into a wide format to reduce redundant information (e.g. repeated time/entity values) sent to the LLM.</p> In\u00a0[15]: Copied! <pre>selection = [\"Aluminum\", \"Banana, Europe\"]\nprices = y_pred.filter(pl.col(entity_col).is_in(selection)).pipe(\n    univariate_panel_to_wide, shrink_dtype=True\n)\n\nprint(\"\ud83d\udcca 'Aluminum' and 'Banana, Europe' prices (wide):\\n\", prices)\n</pre> selection = [\"Aluminum\", \"Banana, Europe\"] prices = y_pred.filter(pl.col(entity_col).is_in(selection)).pipe(     univariate_panel_to_wide, shrink_dtype=True )  print(\"\ud83d\udcca 'Aluminum' and 'Banana, Europe' prices (wide):\\n\", prices) <pre>\ud83d\udcca 'Aluminum' and 'Banana, Europe' prices (wide):\n shape: (30, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 Aluminum    \u2506 Banana, Europe \u2502\n\u2502 ---                 \u2506 ---         \u2506 ---            \u2502\n\u2502 datetime[\u03bcs]        \u2506 f32         \u2506 f32            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-10-01 00:00:00 \u2506 1575.267944 \u2506 0.868          \u2502\n\u2502 2020-11-01 00:00:00 \u2506 1588.387939 \u2506 0.846          \u2502\n\u2502 2020-12-01 00:00:00 \u2506 1602.702026 \u2506 0.824          \u2502\n\u2502 2021-01-01 00:00:00 \u2506 1583.288086 \u2506 0.824          \u2502\n\u2502 \u2026                   \u2506 \u2026           \u2506 \u2026              \u2502\n\u2502 2022-12-01 00:00:00 \u2506 1343.609985 \u2506 1.186          \u2502\n\u2502 2023-01-01 00:00:00 \u2506 1343.609985 \u2506 1.144          \u2502\n\u2502 2023-02-01 00:00:00 \u2506 1396.969971 \u2506 1.126          \u2502\n\u2502 2023-03-01 00:00:00 \u2506 1400.67395  \u2506 1.08           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> In\u00a0[16]: Copied! <pre>analysis = prices.llm.analyze(context=dataset_context)  # This may take a few seconds\nprint(\"\ud83d\udcca Analysis:\\n\", analysis)\n</pre> analysis = prices.llm.analyze(context=dataset_context)  # This may take a few seconds print(\"\ud83d\udcca Analysis:\\n\", analysis) <pre>\ud83d\udcca Analysis:\n - The Aluminum price shows a decreasing trend from October 2020 (1575.27 USD) to March 2021 (1385.47 USD), followed by a slight increase until March 2023 (1400.67 USD).\n- Banana prices in Europe exhibit a fluctuating trend with no clear direction. There is no significant change in prices between October 2020 (0.868 USD) and October 2021 (0.86 USD). However, from October 2021 to March 2023, there is a gradual decline in prices, reaching 1.08 USD.\n- The Aluminum price experienced a significant drop in February 2021, with a decrease of 6.88% compared to the previous month.\n- In contrast, Banana prices in Europe had a small drop in February 2021, with a decrease of 2.36% compared to the previous month.\n- Anomalies in the Aluminum price are observed in February 2021 and May 2021, with decreases of 6.88% and 3.08% respectively, compared to the previous month.\n- Banana prices in Europe show an anomaly in October 2021, with an increase of 5.58% compared to the previous month.\n- Seasonality is not evident in the Aluminum price as the fluctuations do not follow a regular pattern over the months.\n- Banana prices in Europe do not exhibit clear seasonality either, with irregular fluctuations throughout the dataset.\n- The highest Aluminum price is recorded in February 2022 (1401.99 USD), while the lowest is observed in March 2022 (1385.47 USD).\n- The highest Banana price in Europe is recorded in June 2022 (1.188 USD), while the lowest is observed in May 2021 (0.806 USD).\n</pre> <p>Let's now compare the previous selection with a new one. We'll refer to these as baskets A and B.</p> In\u00a0[17]: Copied! <pre>basket_a = prices\nselection_b = [\"Chicken\", \"Cocoa\"]\nbasket_b = y_pred.filter(pl.col(entity_col).is_in(selection_b)).pipe(\n    univariate_panel_to_wide, shrink_dtype=True\n)\n\nprint(\"\ud83d\udcca Basket A -- 'Aluminum' and 'Banana, Europe' (wide):\\n\", basket_a)\nprint(\"\ud83d\udcca Basket B -- 'Chicken' and 'Cocoa' (wide):\\n\", basket_b)\n</pre> basket_a = prices selection_b = [\"Chicken\", \"Cocoa\"] basket_b = y_pred.filter(pl.col(entity_col).is_in(selection_b)).pipe(     univariate_panel_to_wide, shrink_dtype=True )  print(\"\ud83d\udcca Basket A -- 'Aluminum' and 'Banana, Europe' (wide):\\n\", basket_a) print(\"\ud83d\udcca Basket B -- 'Chicken' and 'Cocoa' (wide):\\n\", basket_b) <pre>\ud83d\udcca Basket A -- 'Aluminum' and 'Banana, Europe' (wide):\n shape: (30, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 Aluminum    \u2506 Banana, Europe \u2502\n\u2502 ---                 \u2506 ---         \u2506 ---            \u2502\n\u2502 datetime[\u03bcs]        \u2506 f32         \u2506 f32            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-10-01 00:00:00 \u2506 1575.267944 \u2506 0.868          \u2502\n\u2502 2020-11-01 00:00:00 \u2506 1588.387939 \u2506 0.846          \u2502\n\u2502 2020-12-01 00:00:00 \u2506 1602.702026 \u2506 0.824          \u2502\n\u2502 2021-01-01 00:00:00 \u2506 1583.288086 \u2506 0.824          \u2502\n\u2502 \u2026                   \u2506 \u2026           \u2506 \u2026              \u2502\n\u2502 2022-12-01 00:00:00 \u2506 1343.609985 \u2506 1.186          \u2502\n\u2502 2023-01-01 00:00:00 \u2506 1343.609985 \u2506 1.144          \u2502\n\u2502 2023-02-01 00:00:00 \u2506 1396.969971 \u2506 1.126          \u2502\n\u2502 2023-03-01 00:00:00 \u2506 1400.67395  \u2506 1.08           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\ud83d\udcca Basket B -- 'Chicken' and 'Cocoa' (wide):\n shape: (30, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 Chicken \u2506 Cocoa \u2502\n\u2502 ---                 \u2506 ---     \u2506 ---   \u2502\n\u2502 datetime[\u03bcs]        \u2506 f32     \u2506 f32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-10-01 00:00:00 \u2506 1.492   \u2506 2.41  \u2502\n\u2502 2020-11-01 00:00:00 \u2506 1.588   \u2506 2.42  \u2502\n\u2502 2020-12-01 00:00:00 \u2506 1.606   \u2506 2.408 \u2502\n\u2502 2021-01-01 00:00:00 \u2506 1.536   \u2506 2.372 \u2502\n\u2502 \u2026                   \u2506 \u2026       \u2506 \u2026     \u2502\n\u2502 2022-12-01 00:00:00 \u2506 1.428   \u2506 2.664 \u2502\n\u2502 2023-01-01 00:00:00 \u2506 1.42    \u2506 2.636 \u2502\n\u2502 2023-02-01 00:00:00 \u2506 1.42    \u2506 2.678 \u2502\n\u2502 2023-03-01 00:00:00 \u2506 1.376   \u2506 2.696 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <p>Now compare!</p> In\u00a0[18]: Copied! <pre>comparison = basket_a.llm.compare(\n    as_label=\"Basket A\", others={\"Basket B\": basket_b}\n)  # This may take a few seconds\nprint(\"\ud83d\udcca Comparison:\\n\", comparison)\n</pre> comparison = basket_a.llm.compare(     as_label=\"Basket A\", others={\"Basket B\": basket_b} )  # This may take a few seconds print(\"\ud83d\udcca Comparison:\\n\", comparison) <pre>\ud83d\udcca Comparison:\n Basket A and Basket B represent two different sets of time series data. We will compare and contrast these data sets in terms of trend, seasonality, and anomalies.\n\n**Trend Analysis:**\n\nFor Basket A, the Aluminum prices show a slight decreasing trend over time, with a decrease of 11.7% from October 2020 to March 2023. On the other hand, Banana prices in Europe show a fluctuating trend with no clear direction, but overall, there is a slight increase of 30.3% during the same period.\n\nFor Basket B, the Chicken prices exhibit a slight increasing trend, with an increase of 4.6% from October 2020 to March 2023. The Cocoa prices, on the other hand, show a relatively stable trend with some fluctuations, but no clear direction.\n\n**Seasonality Analysis:**\n\nBasket A does not exhibit any clear seasonality patterns in either Aluminum or Banana prices. The prices seem to fluctuate randomly without any consistent seasonal patterns.\n\nBasket B also does not show any significant seasonality patterns in Chicken or Cocoa prices. The prices vary without following a specific seasonal trend.\n\n**Anomaly Analysis:**\n\nBasket A does not have any obvious anomalies in the Aluminum prices. However, in the Banana prices, there is a significant anomaly in November 2021, where the price jumps by 6.3% compared to the previous month. This anomaly could be due to factors such as supply disruptions or changes in demand.\n\nBasket B does not show any clear anomalies in either Chicken or Cocoa prices. The prices fluctuate within a relatively stable range without any sudden or unexpected changes.\n\nIn summary, Basket A and Basket B exhibit different trends over time. Basket A shows a decreasing trend in Aluminum prices and a fluctuating trend in Banana prices. Basket B shows an increasing trend in Chicken prices and a relatively stable trend in Cocoa prices. Both baskets do not display any clear seasonality patterns. Basket A has an anomaly in November 2021 in Banana prices, while Basket B does not show any significant anomalies.\n</pre>"},{"location":"notebooks/llm/#llm-interoperability","title":"LLM Interoperability\u00b6","text":"<p>This walkthrough covers the <code>functime.llm</code> module, which contains namespaced polars dataframe methods to interoperate Large Language Models (LLMs) with functime.</p>"},{"location":"notebooks/llm/#load-data","title":"Load data\u00b6","text":""},{"location":"notebooks/llm/#analyze-forecasts","title":"Analyze Forecasts\u00b6","text":""},{"location":"notebooks/llm/#compare-forecasts","title":"Compare Forecasts\u00b6","text":""},{"location":"ref/cross-validation/","title":"cross_validation","text":""},{"location":"ref/cross-validation/#functime.cross_validation.expanding_window_split","title":"<code>expanding_window_split(test_size, n_splits=5, step_size=1, eager=False)</code>","text":"<p>Return train/test splits using expanding window splitter.</p> <p>Split time series repeatedly into an growing training set and a fixed-size test set. For example, given <code>test_size = 3</code>, <code>n_splits = 5</code> and <code>step_size = 1</code>, the train <code>o</code>s and test <code>x</code>s folds can be visualized as:</p> <pre><code>| o o o x x x - - - - |\n| o o o o x x x - - - |\n| o o o o o x x x - - |\n| o o o o o o x x x - |\n| o o o o o o o x x x |\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>test_size</code> <code>int</code> <p>Number of test samples for each split.</p> required <code>n_splits</code> <code>int</code> <p>Number of splits.</p> <code>5</code> <code>step_size</code> <code>int</code> <p>Step size between windows.</p> <code>1</code> <code>eager</code> <code>bool</code> <p>If True return DataFrames. Otherwise, return LazyFrames.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>splitter</code> <code>Callable[LazyFrame, Mapping[int, Tuple[LazyFrame, LazyFrame]]]</code> <p>Function that takes a panel LazyFrame and Dict of (train, test) splits, where the key represents the split number (1,2,...,n_splits) and the value is a tuple of LazyFrames.</p>"},{"location":"ref/cross-validation/#functime.cross_validation.sliding_window_split","title":"<code>sliding_window_split(test_size, n_splits=5, step_size=1, window_size=10, eager=False)</code>","text":"<p>Return train/test splits using sliding window splitter. Split time series repeatedly into a fixed-length training and test set. For example, given <code>test_size = 3</code>, <code>n_splits = 5</code>, <code>step_size = 1</code> and <code>window_size=5</code> the train <code>o</code>s and test <code>x</code>s folds can be visualized as:</p> <pre><code>| o o o o o x x x - - - - |\n| - o o o o o x x x - - - |\n| - - o o o o o x x x - - |\n| - - - o o o o o x x x - |\n| - - - - o o o o o x x x |\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>test_size</code> <code>int</code> <p>Number of test samples for each split.</p> required <code>n_splits</code> <code>int</code> <p>Number of splits.</p> <code>5</code> <code>step_size</code> <code>int</code> <p>Step size between windows.</p> <code>1</code> <code>window_size</code> <code>int</code> <p>Window size for training.</p> <code>10</code> <code>eager</code> <code>bool</code> <p>If True return DataFrames. Otherwise, return LazyFrames.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>splitter</code> <code>Callable[LazyFrame, Mapping[int, Tuple[LazyFrame, LazyFrame]]]</code> <p>Function that takes a panel LazyFrame and Dict of (train, test) splits, where the key represents the split number (1,2,...,n_splits) and the value is a tuple of LazyFrames.</p>"},{"location":"ref/cross-validation/#functime.cross_validation.train_test_split","title":"<code>train_test_split(test_size, eager=False)</code>","text":"<p>Return a time-ordered train set and test set given <code>test_size</code>.</p> <p>Parameters:</p> Name Type Description Default <code>test_size</code> <code>int</code> <p>Number of test samples.</p> required <code>eager</code> <code>bool</code> <p>If True, evaluate immediately and returns tuple of train-test <code>DataFrame</code>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>splitter</code> <code>Callable[LazyFrame, Tuple[LazyFrame, LazyFrame]]</code> <p>Function that takes a panel LazyFrame and returns tuple of train / test LazyFrames.</p>"},{"location":"ref/evaluation/","title":"evaluation","text":""},{"location":"ref/evaluation/#functime.evaluation.rank_fva","title":"<code>rank_fva(y_true, y_pred, y_pred_bench=None, scoring=None, descending=False)</code>","text":"<p>Sorts point forecasts in <code>y_pred</code> across entities / time-series by score.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>DataFrame</code> <p>Panel DataFrame of observed values.</p> required <code>y_pred</code> <code>DataFrame</code> <p>Panel DataFrame of point forecasts.</p> required <code>y_pred_bench</code> <code>DataFrame</code> <p>Panel DataFrame of benchmark forecast values.</p> <code>None</code> <code>scoring</code> <code>Optional[metric]</code> <p>If None, defaults to SMAPE.</p> <code>None</code> <code>descending</code> <code>bool</code> <p>Sort in descending order. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ranks</code> <code>DataFrame</code> <p>Cross-sectional DataFrame with two columns: entity name and score.</p>"},{"location":"ref/evaluation/#functime.evaluation.rank_point_forecasts","title":"<code>rank_point_forecasts(y_true, y_pred, sort_by='smape', descending=False)</code>","text":"<p>Sorts point forecasts in <code>y_pred</code> across entities / time-series by score.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>DataFrame</code> <p>Panel DataFrame of observed values.</p> required <code>y_pred</code> <code>DataFrame</code> <p>Panel DataFrame of point forecasts.</p> required <code>sort_by</code> <code>str</code> <p>Metric name.</p> <code>'smape'</code> <code>descending</code> <code>bool</code> <p>Sort in descending order. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ranks</code> <code>DataFrame</code> <p>Cross-sectional DataFrame with two columns: entity name and score.</p>"},{"location":"ref/evaluation/#functime.evaluation.rank_residuals","title":"<code>rank_residuals(y_resids, sort_by='abs_bias', alpha=0.05, descending=False)</code>","text":"<p>Sorts point forecasts in <code>y_pred</code> across entities / time-series by score.</p> <p>Parameters:</p> Name Type Description Default <code>y_resids</code> <code>DataFrame</code> <p>Panel DataFrame of residuals by splits.</p> required <code>sort_by</code> <code>str</code> <p>Method to sort residuals by: <code>bias</code>, <code>abs_bias</code> (absolute bias), <code>normality</code> (via skew and kurtosis tests), or <code>autocorr</code> (Ljung-box test for lag = 1). Defaults to <code>abs_bias</code>.</p> <code>'abs_bias'</code> <code>max_lags</code> <code>int</code> <p>Number of lags to test.</p> required <code>alpha</code> <code>float</code> <p>To compute (1.0 - alpha) confidence interval.</p> <code>0.05</code> <code>descending</code> <code>bool</code> <p>Sort in descending order. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ranks</code> <code>DataFrame</code> <p>Cross-sectional DataFrame with two columns: entity name and score.</p>"},{"location":"ref/feature-extraction/","title":"feature_extraction","text":""},{"location":"ref/feature-extraction/#functime.feature_extraction.add_calendar_effects","title":"<code>add_calendar_effects(attrs, as_dummies=False)</code>","text":"<p>Extract calendar effects from time column, returns calendar effects as categorical columns.</p> <p>Parameters:</p> Name Type Description Default <code>attrs</code> <code>list of str</code> <p>List of calendar effects to be applied to the time column:</p> <ul> <li>\"minute\"</li> <li>\"hour\"</li> <li>\"day\"</li> <li>\"weekday\"</li> <li>\"week\"</li> <li>\"month\"</li> <li>\"quarter\"</li> <li>\"year\"</li> </ul> required <code>as_dummies</code> <code>bool</code> <p>Returns calendar effects as columns of one-hot-encoded dummies.</p> <code>False</code>"},{"location":"ref/feature-extraction/#functime.feature_extraction.add_fourier_terms","title":"<code>add_fourier_terms(sp, K)</code>","text":"<p>Fourier features for time series seasonality.</p> <p>Fourier Series terms can be used as explanatory variables for the cases of multiple seasonal periods and or complex / long seasonal periods.</p> <p>The implementation is based on the Fourier function from the R forecast package.</p> <p>Parameters:</p> Name Type Description Default <code>sp</code> <code>int</code> <p>Seasonal period.</p> required <code>K</code> <code>int</code> <p>Maximum order(s) of Fourier terms. Must be less than <code>sp</code>.</p> required"},{"location":"ref/feature-extraction/#functime.feature_extraction.add_holiday_effects","title":"<code>add_holiday_effects(country_codes, as_dummies=False)</code>","text":"<p>Extract holiday effects from time column for specified ISO-2 country codes and frequency.</p> <p>Parameters:</p> Name Type Description Default <code>country_codes</code> <code>List[str]</code> <p>A list of ISO-2 country codes.</p> required <code>as_dummies</code> <code>bool</code> <p>Returns calendar effects as columns of one-hot-encoded dummies.</p> <code>False</code>"},{"location":"ref/forecasting/","title":"forecasting","text":"<p><code>functime</code> supports both individual forecasters and forecasters with automated lags / hyperparameter tuning. Auto-forecasters uses <code>FLAML</code> to optimize both hyperparameters and number of lagged dependent variables. <code>FLAML</code> is a SOTA library for automated hyperparameter tuning using the CFO (Frugal Optimization for Cost-related Hyperparamters<sup>1</sup>) algorithm. All individual forecasters (e.g. <code>lasso</code> / <code>xgboost</code>) and automated forecasters (e.g. <code>auto_lasso</code> and <code>auto_xgboost</code>) implement the following API.</p>"},{"location":"ref/forecasting/#forecaster","title":"<code>forecaster</code>","text":"<p>Autoregressive forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>str</code> <p>Offset alias supported by Polars.</p> required <code>lags</code> <code>int</code> <p>Number of lagged target variables.</p> required <code>max_horizons</code> <code>Optional[int]</code> <p>Maximum number of horizons to predict directly. Only applied if <code>strategy</code> equals \"direct\" or \"ensemble\".</p> <code>None</code> <code>strategy</code> <code>Optional[str]</code> <p>Forecasting strategy. Currently supports \"recursive\", \"direct\", and \"ensemble\" of both recursive and direct strategies.</p> <code>None</code> <code>target_transform</code> <code>Optional[Transformer]</code> <p>functime transformer to apply to <code>y</code> before fit. The transform is inverted at predict time.</p> <code>None</code> <code>feature_transform</code> <code>Optional[Transformer]</code> <p>functime transformer to apply to <code>X</code> before fit and predict.</p> <code>None</code> <code>**kwargs</code> <code>Mapping[str, Any]</code> <p>Additional keyword arguments passed into underlying sklearn-compatible regressor.</p> <code>{}</code>"},{"location":"ref/forecasting/#auto_forecaster","title":"<code>auto_forecaster</code>","text":"<p>Forecaster with automated hyperparameter tuning and lags selection.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>str</code> <p>Offset alias as dictated.</p> required <code>min_lags</code> <code>int</code> <p>Minimum number of lagged target values.</p> <code>3</code> <code>max_lags</code> <code>int</code> <p>Maximum number of lagged target values.</p> <code>12</code> <code>max_horizons</code> <code>Optional[int]</code> <p>Maximum number of horizons to predict directly. Only applied if <code>strategy</code> equals \"direct\" or \"ensemble\".</p> <code>None</code> <code>strategy</code> <code>Optional[str]</code> <p>Forecasting strategy. Currently supports \"recursive\", \"direct\", and \"ensemble\" of both recursive and direct strategies.</p> <code>None</code> <code>test_size</code> <code>int</code> <p>Number of lags.</p> <code>1</code> <code>step_size</code> <code>int</code> <p>Step size between backtest windows.</p> <code>1</code> <code>n_splits</code> <code>int</code> <p>Number of backtest splits.</p> <code>5</code> <code>time_budget</code> <code>int</code> <p>Maximum time budgeted to train each forecaster per window and set of hyperparameters.</p> <code>5</code> <code>search_space</code> <code>Optional[dict]</code> <p>Equivalent to <code>config</code> in FLAML</p> <code>None</code> <code>points_to_evaluate</code> <code>Optional[dict]</code> <p>Equivalent to <code>points_to_evaluate</code> in FLAML</p> <code>None</code> <code>num_samples</code> <code>int</code> <p>Number of hyper-parameter sets to test. -1 means unlimited (until <code>time_budget</code> is exhausted.)</p> <code>-1</code> <code>target_transform</code> <code>Optional[Transformer]</code> <p>functime transformer to apply to <code>y</code> before fit. The transform is inverted at predict time.</p> <code>None</code> <code>feature_transform</code> <code>Optional[Transformer]</code> <p>functime transformer to apply to <code>X</code> before fit and predict.</p> <code>None</code> <code>**kwargs</code> <code>Mapping[str, Any]</code> <p>Additional keyword arguments passed into underlying sklearn-compatible regressor.</p> <code>{}</code> <p><code>functime</code> also has the following benchmark models implemented as pure Polars queries.</p> <ol> <li> <p>https://arxiv.org/abs/2005.01571\u00a0\u21a9</p> </li> </ol>"},{"location":"ref/forecasting/#functime.forecasting.naive.naive","title":"<code>naive</code>","text":"<p>Naive forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>str</code> <p>Offset alias supported by Polars.</p> required"},{"location":"ref/forecasting/#functime.forecasting.snaive.snaive","title":"<code>snaive</code>","text":"<p>Seasonal naive forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>str</code> <p>Offset alias supported by Polars.</p> required <code>sp</code> <code>int</code> <p>Seasonal periods.</p> required"},{"location":"ref/metrics/","title":"metrics","text":"<p>--&gt;</p>"},{"location":"ref/metrics/#functime.metrics.point.mae","title":"<code>mae(y_true, y_pred)</code>","text":"<p>Return mean absolute error (MAE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>DataFrame</code> <p>Ground truth (correct) target values.</p> required <code>y_pred</code> <code>DataFrame</code> <p>Predicted values.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>DataFrame</code> <p>Score per series.</p>"},{"location":"ref/metrics/#functime.metrics.point.mape","title":"<code>mape(y_true, y_pred)</code>","text":"<p>Return mean absolute percentage error (MAPE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>DataFrame</code> <p>Ground truth (correct) target values.</p> required <code>y_pred</code> <code>DataFrame</code> <p>Predicted values.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>DataFrame</code> <p>Score per series.</p>"},{"location":"ref/metrics/#functime.metrics.point.mase","title":"<code>mase(y_true, y_pred, y_train, sp=1)</code>","text":"<p>Return mean absolute scaled error (MASE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>DataFrame</code> <p>Ground truth (correct) target values.</p> required <code>y_pred</code> <code>DataFrame</code> <p>Predicted values.</p> required <code>y_train</code> <code>DataFrame</code> <p>Observed training values.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>DataFrame</code> <p>Score per series.</p>"},{"location":"ref/metrics/#functime.metrics.point.mfe","title":"<code>mfe(y_true, y_pred)</code>","text":"<p>Return mean forecast error (MFE) AKA bias.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>DataFrame</code> <p>Ground truth (correct) target values.</p> required <code>y_pred</code> <code>DataFrame</code> <p>Predicted values.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>DataFrame</code> <p>Score per series.</p>"},{"location":"ref/metrics/#functime.metrics.point.mse","title":"<code>mse(y_true, y_pred)</code>","text":"<p>Return mean squared error (MSE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>DataFrame</code> <p>Ground truth (correct) target values.</p> required <code>y_pred</code> <code>DataFrame</code> <p>Predicted values.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>DataFrame</code> <p>Score per series.</p>"},{"location":"ref/metrics/#functime.metrics.point.overforecast","title":"<code>overforecast(y_true, y_pred)</code>","text":"<p>Return total overforecast.</p> <p>Overforecast (positive forecast bias) is the difference between actual and predicted for predicted values greater than actual.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>DataFrame</code> <p>Ground truth (correct) target values.</p> required <code>y_pred</code> <code>DataFrame</code> <p>Predicted values.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>DataFrame</code> <p>Score per series.</p>"},{"location":"ref/metrics/#functime.metrics.point.rmse","title":"<code>rmse(y_true, y_pred)</code>","text":"<p>Return root mean squared error (RMSE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>DataFrame</code> <p>Ground truth (correct) target values.</p> required <code>y_pred</code> <code>DataFrame</code> <p>Predicted values.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>DataFrame</code> <p>Score per series.</p>"},{"location":"ref/metrics/#functime.metrics.point.rmsse","title":"<code>rmsse(y_true, y_pred, y_train, sp=1)</code>","text":"<p>Return root mean squared scaled error (RMSSE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>DataFrame</code> <p>Ground truth (correct) target values.</p> required <code>y_pred</code> <code>DataFrame</code> <p>Predicted values.</p> required <code>y_train</code> <code>DataFrame</code> <p>Observed training values.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>DataFrame</code> <p>Score per series.</p>"},{"location":"ref/metrics/#functime.metrics.point.smape","title":"<code>smape(y_true, y_pred)</code>","text":"<p>Return symmetric mean absolute percentage error (sMAPE).</p> <p>Use third version of SMAPE formula from https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error to deal with zero division error</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>DataFrame</code> <p>Ground truth (correct) target values.</p> required <code>y_pred</code> <code>DataFrame</code> <p>Predicted values.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>DataFrame</code> <p>Score per series.</p>"},{"location":"ref/metrics/#functime.metrics.point.smape_original","title":"<code>smape_original(y_true, y_pred)</code>","text":"<p>Return symmetric mean absolute percentage error (sMAPE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>DataFrame</code> <p>Ground truth (correct) target values.</p> required <code>y_pred</code> <code>DataFrame</code> <p>Predicted values.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>DataFrame</code> <p>Score per series.</p>"},{"location":"ref/metrics/#functime.metrics.point.underforecast","title":"<code>underforecast(y_true, y_pred)</code>","text":"<p>Return total underforecast.</p> <p>Underforecast (negative forecast bias) is the difference between actual and predicted for predicted values less than actual.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>DataFrame</code> <p>Ground truth (correct) target values.</p> required <code>y_pred</code> <code>DataFrame</code> <p>Predicted values.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>DataFrame</code> <p>Score per series.</p>"},{"location":"ref/multi-objective/","title":"multi_objective","text":"<p>Module with functions to compute, compare, and optimize multi-objective forecasts.</p>"},{"location":"ref/multi-objective/#functime.metrics.multi_objective.score_backtest","title":"<code>score_backtest(y_true, y_preds, agg_method=None)</code>","text":"<p>Return DataFrame of forecast metrics across entities.</p> <p>Metrics returned:</p> <ul> <li>MAE</li> <li>MASE</li> <li>MSE</li> <li>Overforecast</li> <li>RMSE</li> <li>RMSSE</li> <li>SMAPE</li> <li>Underforecast</li> </ul> <p>Note: MAPE is excluded to avoid potential divide by zero errors. We recommend looking at SMAPE instead.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>DataFrame</code> <p>Ground truth (correct) target values.</p> required <code>y_preds</code> <code>DataFrame</code> <p>Stacked predicted values across CV splits. DataFrame contains four columns: entity, time, target, \"split\".</p> required <code>agg_method</code> <code>Optional[str] = None</code> <p>Method (\"mean\", \"median\") to aggregate scores across entities by. If None, forecasts in overlapping splits are weighted equally, i.e. no aggregation is applied.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>DataFrame</code> <p>DataFrame with computed metrics column by column across entities row by row.</p>"},{"location":"ref/multi-objective/#functime.metrics.multi_objective.score_forecast","title":"<code>score_forecast(y_true, y_pred, y_train)</code>","text":"<p>Return DataFrame of forecast metrics across entities.</p> <p>Metrics returned:</p> <ul> <li>MAE</li> <li>MASE</li> <li>MSE</li> <li>Overforecast</li> <li>RMSE</li> <li>RMSSE</li> <li>SMAPE</li> <li>Underforecast</li> </ul> <p>Note: SMAPE is used instead of MAPE to avoid potential divide by zero errors.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>DataFrame</code> <p>Ground truth (correct) target values.</p> required <code>y_pred</code> <code>DataFrame</code> <p>Predicted values.</p> required <code>y_train</code> <code>DataFrame</code> <p>Observed training values.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>DataFrame</code> <p>DataFrame with computed metrics column by column across entities row by row.</p>"},{"location":"ref/multi-objective/#functime.metrics.multi_objective.summarize_scores","title":"<code>summarize_scores(scores, agg_method='mean')</code>","text":"<p>Given a DataFrame of forecast metrics, return a dataclass of metrics aggregated by <code>agg_method</code>.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>DataFrame</code> <p>DataFrame of scores. N rows of entities by M columns of metrics.</p> required <code>agg_method</code> <code>str</code> <p>Method (\"mean\", \"median\") to aggregate scores across entities by.</p> <code>'mean'</code> <p>Returns:</p> Name Type Description <code>metrics</code> <code>Metrics</code> <p>Dataclass of scores aggregated across entities.</p>"},{"location":"ref/offsets/","title":"offsets","text":""},{"location":"ref/offsets/#functime.offsets.freq_to_sp","title":"<code>freq_to_sp(freq)</code>","text":"<p>Return seasonal periods given offset alias.</p> <p>Reference: https://robjhyndman.com/hyndsight/seasonal-periods/</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>str</code> <p>Supported offset aliases:</p> <ul> <li>1s (1 second)</li> <li>1m (1 minute)</li> <li>30m (30 minute)</li> <li>1h (1 hour)</li> <li>1d (1 day)</li> <li>1w (1 week)</li> <li>1mo (1 calendar month)</li> <li>3mo (1 calendar quarter)</li> <li>1y (1 calendar year)</li> </ul> required <p>Returns:</p> Name Type Description <code>sp</code> <code>list of int</code>"},{"location":"ref/plotting/","title":"plotting","text":""},{"location":"ref/plotting/#functime.plotting.plot_backtests","title":"<code>plot_backtests(y_true, y_preds, n_cols=2, last_n=DEFAULT_LAST_N, **kwargs)</code>","text":"<p>Given panel DataFrame of observed values <code>y</code> and backtests across splits <code>y_pred</code>, returns subplots for each individual entity / time-series.</p> <p>Note: if you have over 10 entities / time-series, we recommend using the <code>rank_</code> functions in <code>functime.evaluation</code> then <code>df.head()</code> before plotting.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>DataFrame</code> <p>Panel DataFrame of observed values.</p> required <code>y_preds</code> <code>DataFrame</code> <p>Panel DataFrame of backtested values.</p> required <code>n_cols</code> <code>int</code> <p>Number of columns to arrange subplots. Defaults to 2.</p> <code>2</code> <code>last_n</code> <code>int</code> <p>Plot <code>last_n</code> most recent values in <code>y</code> and <code>y_pred</code>. Defaults to 64.</p> <code>DEFAULT_LAST_N</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>Plotly subplots.</p>"},{"location":"ref/plotting/#functime.plotting.plot_comet","title":"<code>plot_comet(y_train, y_test, y_pred, scoring=None, **kwargs)</code>","text":"<p>Given a train-test-split of panel data (<code>y_train</code>, <code>y_test</code>) and forecast <code>y_pred</code>, returns a Comet plot i.e. scatterplot of volatility per entity in <code>y_train</code> against the forecast scores.</p> <p>Parameters:</p> Name Type Description Default <code>y_train</code> <code>DataFrame</code> <p>Panel DataFrame of train dataset.</p> required <code>y_test</code> <code>DataFrame</code> <p>Panel DataFrame of test dataset.</p> required <code>y_pred</code> <code>DataFrame</code> <p>Panel DataFrame of forecasted values to score against <code>y_test</code>.</p> required <code>scoring</code> <code>Optional[metric]</code> <p>If None, defaults to SMAPE.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>Plotly scatterplot.</p>"},{"location":"ref/plotting/#functime.plotting.plot_forecasts","title":"<code>plot_forecasts(y_true, y_pred, n_cols=2, last_n=DEFAULT_LAST_N, **kwargs)</code>","text":"<p>Given panel DataFrames of observed values <code>y</code> and forecasts <code>y_pred</code>, returns subplots for each individual entity / time-series.</p> <p>Note: if you have over 10 entities / time-series, we recommend using the <code>rank_</code> functions in <code>functime.evaluation</code> then <code>df.head()</code> before plotting.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>DataFrame</code> <p>Panel DataFrame of observed values.</p> required <code>y_pred</code> <code>DataFrame</code> <p>Panel DataFrame of forecasted values.</p> required <code>n_cols</code> <code>int</code> <p>Number of columns to arrange subplots. Defaults to 2.</p> <code>2</code> <code>last_n</code> <code>int</code> <p>Plot <code>last_n</code> most recent values in <code>y</code> and <code>y_pred</code>. Defaults to 64.</p> <code>DEFAULT_LAST_N</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>Plotly subplots.</p>"},{"location":"ref/plotting/#functime.plotting.plot_fva","title":"<code>plot_fva(y_true, y_pred, y_pred_bench, scoring=None, **kwargs)</code>","text":"<p>Given two panel data forecasts <code>y_pred</code> and <code>y_pred_bench</code>, returns scatterplot of benchmark scores against forecast scores. Each dot represents a single entity / time-series.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>DataFrame</code> <p>Panel DataFrame of test dataset.</p> required <code>y_pred</code> <code>DataFrame</code> <p>Panel DataFrame of forecasted values.</p> required <code>y_pred_bench</code> <code>DataFrame</code> <p>Panel DataFrame of benchmark forecast values.</p> required <code>scoring</code> <code>Optional[metric]</code> <p>If None, defaults to SMAPE.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>Plotly scatterplot.</p>"},{"location":"ref/plotting/#functime.plotting.plot_residuals","title":"<code>plot_residuals(y_resids, n_bins=None, **kwargs)</code>","text":"<p>Given panel DataFrame of residuals across splits <code>y_resids</code>, returns binned counts plot of forecast residuals colored by entity / time-series.</p> <p>Useful for residuals analysis (bias and normality) at scale.</p> <p>Parameters:</p> Name Type Description Default <code>y_resids</code> <code>DataFrame</code> <p>Panel DataFrame of forecast residuals (i.e. observed less forecast).</p> required <code>n_bins</code> <code>int</code> <p>Number of bins.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>figure</code> <code>Figure</code> <p>Plotly histogram.</p>"},{"location":"ref/preprocessing/","title":"preprocessing","text":""},{"location":"ref/preprocessing/#functime.preprocessing.boxcox","title":"<code>boxcox(method='mle')</code>","text":"<p>Applies the Box-Cox transformation to numeric columns in a panel DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The method used to determine the lambda parameter of the Box-Cox transformation.</p> <p>Supported methods:</p> <ul> <li><code>mle</code>: maximum likelihood estimation</li> <li><code>pearsonr</code>: Pearson correlation coefficient</li> </ul> <code>'mle'</code>"},{"location":"ref/preprocessing/#functime.preprocessing.coerce_dtypes","title":"<code>coerce_dtypes(schema)</code>","text":"<p>Coerces the column datatypes of a DataFrame using the provided schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Mapping[str, DataType]</code> <p>A dictionary-like object mapping column names to the desired data types.</p> required"},{"location":"ref/preprocessing/#functime.preprocessing.detrend","title":"<code>detrend(method='linear')</code>","text":"<p>Removes mean or linear trend from numeric columns in a panel DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>If <code>mean</code>, subtracts mean from each time-series. If <code>linear</code>, subtracts line of best-fit (via OLS) from each time-series. Defaults to <code>linear</code>.</p> <code>'linear'</code>"},{"location":"ref/preprocessing/#functime.preprocessing.diff","title":"<code>diff(order, sp=1)</code>","text":"<p>Difference time-series in panel data given order and seasonal period.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>int</code> <p>The order to difference.</p> required <code>sp</code> <code>int</code> <p>Seasonal periodicity.</p> <code>1</code>"},{"location":"ref/preprocessing/#functime.preprocessing.impute","title":"<code>impute(method)</code>","text":"<p>Performs missing value imputation on numeric columns of a DataFrame grouped by entity.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Union[str, int, float]</code> <p>The imputation method to use.</p> <p>Supported methods are:</p> <ul> <li>'mean': Replace missing values with the mean of the corresponding column.</li> <li>'median': Replace missing values with the median of the corresponding column.</li> <li>'fill': Replace missing values with the mean for float columns and the median for integer columns.</li> <li>'ffill': Forward fill missing values.</li> <li>'bfill': Backward fill missing values.</li> <li>'interpolate': Interpolate missing values using linear interpolation.</li> <li>int or float: Replace missing values with the specified constant.</li> </ul> required"},{"location":"ref/preprocessing/#functime.preprocessing.lag","title":"<code>lag(lags)</code>","text":"<p>Applies lag transformation to a LazyFrame.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>List[int]</code> <p>A list of lag values to apply.</p> required"},{"location":"ref/preprocessing/#functime.preprocessing.one_hot_encode","title":"<code>one_hot_encode(drop_first=False)</code>","text":"<p>Encode categorical features as a one-hot numeric array.</p> <p>Parameters:</p> Name Type Description Default <code>drop_first</code> <code>bool</code> <p>Drop the first one hot feature.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if X passed into <code>transform_new</code> contains unknown categories.</p>"},{"location":"ref/preprocessing/#functime.preprocessing.reindex","title":"<code>reindex(drop_duplicates=False)</code>","text":"<p>Reindexes the entity and time columns to have every possible combination of (entity, time).</p> <p>Parameters:</p> Name Type Description Default <code>drop_duplicates</code> <code>bool</code> <p>Defaults to False. If True, duplicates are dropped before reindexing.</p> <code>False</code>"},{"location":"ref/preprocessing/#functime.preprocessing.resample","title":"<code>resample(freq, agg_method, impute_method)</code>","text":"<p>Resamples and transforms a DataFrame using the specified frequency, aggregation method, and imputation method.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>str</code> <p>Offset alias supported by Polars.</p> required <code>agg_method</code> <code>str</code> <p>The aggregation method to use for resampling. Supported values are 'sum', 'mean', and 'median'.</p> required <code>impute_method</code> <code>Union[str, int, float]</code> <p>The method used for imputing missing values. If a string, supported values are 'ffill' (forward fill) and 'bfill' (backward fill). If an int or float, missing values will be filled with the provided value.</p> required"},{"location":"ref/preprocessing/#functime.preprocessing.roll","title":"<code>roll(window_sizes, stats, freq)</code>","text":"<p>Performs rolling window calculations on specified columns of a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>window_sizes</code> <code>List[int]</code> <p>A list of integers representing the window sizes for the rolling calculations.</p> required <code>stats</code> <code>List[Literal['mean', 'min', 'max', 'mlm', 'sum', 'std', 'cv']]</code> <p>A list of statistical measures to calculate for each rolling window.</p> <p>Supported values are:</p> <ul> <li>'mean' for mean</li> <li>'min' for minimum</li> <li>'max' for maximum</li> <li>'mlm' for maximum minus minimum</li> <li>'sum' for sum</li> <li>'std' for standard deviation</li> <li>'cv' for coefficient of variation</li> </ul> required <code>freq</code> <code>str</code> <p>Offset alias supported by Polars.</p> required"},{"location":"ref/preprocessing/#functime.preprocessing.scale","title":"<code>scale(use_mean=True, use_std=True, rescale_bool=False)</code>","text":"<p>Performs scaling and rescaling operations on the numeric columns of a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>use_mean</code> <code>bool</code> <p>Whether to subtract the mean from the numeric columns. Defaults to True.</p> <code>True</code> <code>use_std</code> <code>bool</code> <p>Whether to divide the numeric columns by the standard deviation. Defaults to True.</p> <code>True</code> <code>rescale_bool</code> <code>bool</code> <p>Whether to rescale boolean columns to the range [-1, 1]. Defaults to False.</p> <code>False</code>"},{"location":"ref/preprocessing/#functime.preprocessing.time_to_arange","title":"<code>time_to_arange(eager=False)</code>","text":"<p>Coerces time column into arange per entity.</p> <p>Assumes even-spaced time-series and homogenous start dates.</p>"},{"location":"ref/preprocessing/#functime.preprocessing.trim","title":"<code>trim(direction='both')</code>","text":"<p>Trims time-series in panel to have the same start or end dates as the shortest time-series.</p> <p>Parameters:</p> Name Type Description Default <code>direction</code> <code>Literal['both', 'left', 'right']</code> <p>Defaults to \"both\". If \"left\" trims from start date of the shortest time series); if \"right\" trims up to the end date of the shortest time-series; or otherwise \"both\" trims between start and end dates of the shortest time-series</p> <code>'both'</code>"}]}