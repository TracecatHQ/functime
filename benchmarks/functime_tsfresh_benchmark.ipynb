{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import perfplot\n",
    "from typing import Union, Callable\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from functime.feature_extraction import tsfresh as f_ts\n",
    "from tsfresh.feature_extraction import feature_calculators as tsfresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variable\n",
    "\n",
    "We are using the M4 dataset. We create a `pd.DataFrame`, `pl.DataFrame` and `pl.LazyFrame`. Then we define a list of dictionnary with the following structure:\n",
    "<br>\n",
    "(<br>\n",
    "&emsp;  `<functime_function_name>`,<br>\n",
    "&emsp;  `<tsfresh_function_name>`,<br>\n",
    "&emsp;  `<parameters_for_functime_function>`,<br>\n",
    "&emsp;   `<parameters_for_tsfresh_function>`<br>\n",
    ")<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_M4_DATASET = \"../data/M4_daily.parquet\"\n",
    "\n",
    "DF_PANDAS = pd.melt(pd.read_parquet(_M4_DATASET)).drop(columns=[\"variable\"]).dropna().reset_index(drop=True)\n",
    "DF_PL_EAGER = pl.from_pandas(DF_PANDAS)\n",
    "DF_PL_LAZY = DF_PL_EAGER.lazy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FUNC_PARAMS_BENCH  = [\n",
    "    (f_ts.absolute_energy, tsfresh.abs_energy, {}, {}),\n",
    "    (f_ts.absolute_maximum, tsfresh.absolute_maximum, {}, {}),\n",
    "    (f_ts.absolute_sum_of_changes, tsfresh.absolute_sum_of_changes, {}, {}),\n",
    "    (f_ts.approximate_entropy, tsfresh.approximate_entropy, {\"run_length\": 2, \"filtering_level\": 0.5}, {\"m\": 2, \"r\": 0.5}),\n",
    "    # (f_ts.augmented_dickey_fuller, tsfresh.augmented_dickey_fuller, \"param\")\n",
    "    (f_ts.autocorrelation, tsfresh.autocorrelation, {\"n_lags\": 4}, {\"lag\": 4}),\n",
    "    (f_ts.autoregressive_coefficients, tsfresh.ar_coefficient, {\"n_lags\": 4}, {\"param\": [{\"coeff\": i, \"k\": 4}] for i in range(5)}),\n",
    "    (f_ts.benford_correlation2, tsfresh.benford_correlation, {}, {}),\n",
    "    (f_ts.benford_correlation, tsfresh.benford_correlation, {}, {}),\n",
    "    (f_ts.binned_entropy, tsfresh.binned_entropy, {\"bin_count\": 10}, {\"max_bins\": 10}),\n",
    "    (f_ts.c3, tsfresh.c3, {\"n_lags\": 10}, {\"lag\": 10}),\n",
    "    (f_ts.change_quantiles, tsfresh.change_quantiles, {\"q_low\": 0.1, \"q_high\": 0.9, \"is_abs\": True}, {\"ql\": 0.1, \"qh\": 0.9, \"isabs\": True, \"f_agg\": \"mean\"}),\n",
    "    (f_ts.cid_ce, tsfresh.cid_ce, {\"normalize\": True}, {\"normalize\": True}),\n",
    "    (f_ts.count_above, tsfresh.count_above, {\"threshold\": 0.0}, {\"t\": 0.0}),\n",
    "    (f_ts.count_above_mean, tsfresh.count_above_mean, {}, {}),\n",
    "    (f_ts.count_below, tsfresh.count_below, {\"threshold\": 0.0}, {\"t\": 0.0}),\n",
    "    (f_ts.count_below_mean, tsfresh.count_below_mean, {}, {}),\n",
    "    # (f_ts.cwt_coefficients, tsfresh.cwt_coefficients, {\"widths\": (1, 2, 3), \"n_coefficients\": 2},{\"param\": {\"widths\": (1, 2, 3), \"coeff\": 2, \"w\": 1}}),\n",
    "    (f_ts.energy_ratios, tsfresh.energy_ratio_by_chunks, {\"n_chunks\": 6}, {\"param\": [{\"num_segments\": 6, \"segment_focus\": i} for i in range(6)]}),\n",
    "    (f_ts.first_location_of_maximum, tsfresh.first_location_of_maximum, {}, {}),\n",
    "    (f_ts.first_location_of_minimum, tsfresh.first_location_of_minimum, {}, {}),\n",
    "    # (f_ts.fourier_entropy, tsfresh.fourier_entropy, {\"n_bins\": 10}, {\"bins\": 10}),\n",
    "    # (f_ts.friedrich_coefficients, tsfresh.friedrich_coefficients, {\"polynomial_order\": 3, \"n_quantiles\": 30}, {\"params\": [{\"m\": 3, \"r\": 30}]}),\n",
    "    (f_ts.has_duplicate, tsfresh.has_duplicate, {}, {}),\n",
    "    (f_ts.has_duplicate_max, tsfresh.has_duplicate_max, {}, {}),\n",
    "    (f_ts.has_duplicate_min, tsfresh.has_duplicate_min, {}, {}),\n",
    "    (f_ts.index_mass_quantile, tsfresh.index_mass_quantile, {\"q\": 0.5}, {\"param\": [{\"q\": 0.5}]}),\n",
    "    (f_ts.large_standard_deviation, tsfresh.large_standard_deviation, {\"ratio\": 0.25}, {\"r\": 0.25}),\n",
    "    (f_ts.last_location_of_maximum, tsfresh.last_location_of_maximum, {}, {}),\n",
    "    (f_ts.last_location_of_minimum, tsfresh.last_location_of_minimum, {}, {}),\n",
    "    # (f_ts.lempel_ziv_complexity, tsfresh.lempel_ziv_complexity, {\"n_bins\": 5}, {\"bins\": 5}),\n",
    "    # (f_ts.linear_trend, tsfresh.linear_trend, {}, {\"params\": [{\"attr\": \"slope\"}, {\"attr\": \"intercept\"}]}),\n",
    "    (f_ts.longest_streak_above_mean, tsfresh.longest_strike_above_mean, {}, {}),\n",
    "    (f_ts.longest_streak_below_mean, tsfresh.longest_strike_below_mean, {}, {}),\n",
    "    (f_ts.mean_abs_change, tsfresh.mean_abs_change, {}, {}),\n",
    "    (f_ts.mean_change, tsfresh.mean_change, {}, {}),\n",
    "    (f_ts.mean_n_absolute_max, tsfresh.mean_n_absolute_max, {\"n_maxima\": 20}, {\"number_of_maxima\": 20}),\n",
    "    (f_ts.mean_second_derivative_central, tsfresh.mean_second_derivative_central, {}, {}),\n",
    "    (f_ts.number_crossings, tsfresh.number_crossing_m, {\"crossing_value\": 0.0}, {\"m\": 0.0}),\n",
    "    (f_ts.number_cwt_peaks, tsfresh.number_cwt_peaks, {\"max_width: 5\"}, {\"n\": 5}),\n",
    "    (f_ts.number_peaks, tsfresh.number_peaks, {\"support\": 5}, {\"n\": 5}),\n",
    "    # (f_ts.partial_autocorrelation, tsfresh.partial_autocorrelation, \"param\"),\n",
    "    (f_ts.percent_reoccurring_values, tsfresh.percentage_of_reoccurring_values_to_all_values, {}, {}),\n",
    "    (f_ts.percent_reoccurring_points, tsfresh.percentage_of_reoccurring_datapoints_to_all_datapoints, {}, {}),\n",
    "    (f_ts.permutation_entropy, tsfresh.permutation_entropy, {\"tau\": 1,\"n_dims\": 3}, {\"tau\": 1,\"dimension\": 3}),\n",
    "    (f_ts.range_count, tsfresh.range_count, {\"lower\": 0, \"upper\": 9, \"closed\": 'none'}, {\"min\": 0, \"max\": 9}),\n",
    "    (f_ts.ratio_beyond_r_sigma, tsfresh.ratio_beyond_r_sigma, {\"ratio\": 2}, {\"r\": 2}),\n",
    "    (f_ts.ratio_n_unique_to_length, tsfresh.ratio_value_number_to_time_series_length, {}, {}),\n",
    "    (f_ts.root_mean_square, tsfresh.root_mean_square, {}, {}),\n",
    "    (f_ts.sample_entropy, tsfresh.sample_entropy, {}, {}),\n",
    "    (f_ts.spkt_welch_density, tsfresh.spkt_welch_density, {\"n_coeffs\": 10}, {\"param\": [{\"coeff\": i} for i in range(10)]}),\n",
    "    (f_ts.sum_reoccurring_points, tsfresh.sum_of_reoccurring_data_points, {}, {}),\n",
    "    (f_ts.sum_reoccurring_values, tsfresh.sum_of_reoccurring_values, {}, {}),\n",
    "    (f_ts.symmetry_looking, tsfresh.symmetry_looking, {\"ratio\": 0.25}, {\"param\": [{\"r\": 0.25}]}),\n",
    "    (f_ts.time_reversal_asymmetry_statistic, tsfresh.time_reversal_asymmetry_statistic, {\"n_lags\": 3}, {\"lag\": 3}),\n",
    "    (f_ts.variation_coefficient, tsfresh.variation_coefficient, {}, {}),\n",
    "    (f_ts.var_gt_std, tsfresh.variance_larger_than_standard_deviation, {}, {})\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark tsfresh vs functime\n",
    "\n",
    "Benchmark function to compare the performance of functime's against tsfresh's' function. You need to provide the content from the list `_FUNC_PARAMS_BENCH`. The time series length is taken for \"n\" from 2^6 to 2^24 (except 2^15 for entropy related features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(f_feat: Callable, ts_feat: Callable, f_params: dict, ts_params: dict, expr: bool):\n",
    "    upper_n = 24\n",
    "    if f_feat.__name__ in (\"binned_entropy\", \"approximate_entropy\", \"permutation_entropy\", \"sample_entropy\"):\n",
    "        upper_n = 15\n",
    "    benchmark = perfplot.bench(\n",
    "        setup = lambda n: (DF_PANDAS.head(n), DF_PL_EAGER.head(n)),\n",
    "        kernels = [\n",
    "            lambda x, _y: ts_feat(x[\"value\"], **ts_params),\n",
    "            lambda _x, y: f_feat(y[\"value\"], **f_params) if not expr else y.select(f_feat(pl.col(\"value\"), **f_params))\n",
    "        ],\n",
    "        n_range = [2**k for k in range(6, upper_n)],\n",
    "        equality_check=False,\n",
    "        labels=[\"tsfresh\", \"tsfresh\", \"functime\"]\n",
    "    )\n",
    "    return benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark tsfresh vs functime for all the functions\n",
    "\n",
    "Loop over `_FUNC_PARAMS_BENCH` and call `benchmark()` for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_benchmarks(params: list[tuple], expr: bool)-> list:\n",
    "    bench_df = pl.DataFrame(\n",
    "        schema={\n",
    "            \"id\": pl.Utf8,\n",
    "            \"n\": pl.Int64,\n",
    "            \"tsfresh\": pl.Float64,\n",
    "            \"functime\": pl.Float64,\n",
    "            \"X_time\": pl.Float64\n",
    "        }\n",
    "    )\n",
    "    for x in params:\n",
    "        try:\n",
    "            print(\"Feature: {}\".format(x[0].__name__))\n",
    "            bench = benchmark(\n",
    "                f_feat = x[0],\n",
    "                ts_feat = x[1],\n",
    "                f_params = x[2],\n",
    "                ts_params = x[3],\n",
    "                expr = expr\n",
    "            )\n",
    "            bench_df = pl.concat([\n",
    "                pl.DataFrame({\n",
    "                    \"id\": [x[0].__name__]*len(bench.n_range),\n",
    "                    \"n\": bench.n_range,\n",
    "                    \"tsfresh\": bench.timings_s[0],\n",
    "                    \"functime\": bench.timings_s[1],\n",
    "                    \"X_time\": bench.timings_s[0] / bench.timings_s[1]                \n",
    "                }),\n",
    "                bench_df]\n",
    "            )\n",
    "        except:\n",
    "            print(\"Failure for feature: {}\".format(x[0].__name__))\n",
    "    return bench_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and save the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_table(df: pl.DataFrame):\n",
    "    df_pivot = df.pivot(index=\"id\", values=\"X_time\", columns=\"n\")\n",
    "    df = df_pivot.with_columns(\n",
    "        avg = df_pivot.mean(axis=1)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_expr = all_benchmarks(params = _FUNC_PARAMS_BENCH, expr = True)\n",
    "bench_series = all_benchmarks(params = _FUNC_PARAMS_BENCH, expr = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expr = benchmark_table(bench_expr)\n",
    "# \"benchmark_tsfresh_vs_functime_expr.parquet\" doesn't work\n",
    "df_expr.write_parquet(\"../benchmarks/benchmark_tsfresh_vs_functime_expr.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series = benchmark_table(bench_series)\n",
    "# \"benchmark_tsfresh_vs_functime_series.parquet\" doesn't work\n",
    "df_series.write_csv(\"../benchmarks/benchmark_tsfresh_vs_functime_series.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
